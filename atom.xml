<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ran&#39;s Homepage</title>
  
  <subtitle>Notes about technology and life.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-01-10T01:09:43.314Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Ran Zhao</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习中的偏差与方差</title>
    <link href="http://yoursite.com/2020/01/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/"/>
    <id>http://yoursite.com/2020/01/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/</id>
    <published>2020-01-09T08:39:33.000Z</published>
    <updated>2020-01-10T01:09:43.314Z</updated>
    
    <content type="html"><![CDATA[<p>对偏差-方差的权衡关系对于理解机器学习模型尤为重要，所谓学习模型是从给定的数据集中找到一个最佳的映射函数$f(x)$，它不仅在已知数据上有优异的表现，同时在未知数据上也要具有最佳预测能力。对于训练好的模型，我们难以直接判断它在未知数据上的预测情况，而偏差-方差的权衡给了我们一个判断的准则。对方差-偏差的理解，也有助于理解如何提高机器学习模型性能的深层机制。</p><a id="more"></a><p>机器学习中，我们使用训练集去训练一个学习模型，通常的做法是定义一个损失函数，通过优化损失函数取得最小值来得到最优模型，但此时的模型并不能保证在未知数据集上可用，此时模型在未知数据上的预测误差即为泛化误差（generalization error），而泛化误差可以分解为：噪声，偏差和方差，讨论如下。</p><p>为了方便表述，我们事先定义如下的符号：</p><ul><li>$x$：测试样本</li><li>$D$：数据集</li><li>$y_D$：样本$x$在数据集$D$中的标记</li><li>$y$：样本$x$的真实标记</li><li>$f$：通过训练集$D$学的的模型</li><li>$f(x;D)$：通过训练集$D$学得的模型对样本$x$的输出</li><li>$\overline f(x)$：模型$f$对样本$x$的期望输出</li></ul><h2>一. 噪声</h2><p>监督学习中，我们预先有一个从真实世界中采样得到的数据集$y_D$，由于采样误差，数据集中的目标变量$y_D$并非真实的标签，我们假设存在一个真实的模型，那么真实模型的结果$y$与数据集$D$中的标签$y_D$之间有一个误差，它是由于采样而带来的噪声，满足均值为0，方差为$\sigma^2$的正态分布：$$ y = y_D + \epsilon $$这里，$\epsilon$是噪声，而学习算法通过对数据集进行损失函数最优求解得到的最佳模型，我们记为：$f(x)$。</p><h2>二. 偏差（bias)</h2><p>期望输出$\overline f(x)$对真实值$y$的误差称为偏差：$$bias^2(x) = (\overline f(x) - y)^2$$</p><h2>三. 方差（var)</h2><p>学习模型$f$对测试样本$x$的期望预测定义为：$$ \overline f(x) = E_D[ f(x;D) ] $$即对不同的数据集$D$预测值的平均期望。这样样本数相同的训练集上预测值的方差定义为：$$ var[f(x)] = E_D \left[ \left(f(x;D) - \overline f(x) \right)^2 \right] $$</p><h2>四. 泛化误差</h2><p>我们使用平方误差函数来衡量模型寻优的程度：$|y - f(x;D)|^2$，模型的期望泛化误差定义为：\begin{align*}E(f;D) &amp;= E_D \left[ \left(f(x;D) - y_D \right)^2 \right] \\&amp;= E_D \left[ \left( f(x;D) - \overline f(x) + \overline f(x) -y_D \right)^2\right] \\&amp;= E_D \left[ \left( f(x;D) - \overline f(x) \right)^2 \right] + E_D \left[ \left( \overline f(x) - y_D \right)^2 \right] \\&amp;+ E_D \left[ \left(f(x;D) - \overline f(x) \right) \left(\overline f(x)-y_D \right) \right]\end{align*}上式中最后一个等式的最后一项为0，对等式的第二项可以以同样的方式化简：\begin{align*}E_D \left[ \left( \overline f(x) - y_D \right)^2 \right]&amp;= E_D \left[ \left( \overline f(x) - y + y - y_D \right)^2 \right] \\&amp;= E_D \left[ \left( \overline f(x) - y \right)^2 \right] + E_D \left[ \left( y - y_D \right)^2 \right] \\&amp;+ E_D \left[ \left( \overline f(x) - y \right) \left( y - y_D \right) \right]\end{align*}</p><p>由于噪声是均值为0的正态分布，即：$y-y_D=0$，上式最后一个等式的最后一项也为0；另方面 $(\overline f(x) -y)^2$与数据集无关，于是整个期望泛化误差可以分解为三项：$$ E(f;D) = E_D \left[ \left( f(x;D) - \overline f(x) \right)^2 \right] + \left( \overline f(x) - y \right)^2 + E_D \left[ \left( y - y_D \right)^2 \right] $$这三项分别对应模型的<strong>方差</strong>，<strong>偏差</strong>，和数据集的<strong>噪声</strong>。</p><ul><li>偏差：度量了模型的期望预测和真实结果的偏离程度，刻画了模型本身的拟合能力。</li><li>方差：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。</li><li>噪声：表达了当前任务上任何模型所能达到的期望泛化误差的下界，刻画了学习问题本身的难度。</li></ul><h2>五. 偏差-方差与模型复杂度，欠拟合，过拟合的关系</h2><p>一般来说，参数模型引入较多的先验假设，模型较为简单，训练时需要的数据量较小，结果通常会有一个较大的偏差和较小的方差，如线性回归，逻辑回归；非参数模型引入假设少，模型复杂，表达能力强，训练时需要大量的数据，结果模型偏差较小而方差较大，如K近邻，SVM等。业界常以下述靶心图来直观的表述模型的方差，偏差。<img src="https://i.loli.net/2020/01/09/KcWN1nAu2X8JBmp.png" width="50%"></p><center>图1：偏差-方差的靶心图描述（左下图对应模型的欠拟合状态，右上图对应模型的过拟合状态）</center><img src="https://i.loli.net/2020/01/09/1gJYfuzRHKs4WXe.png" width="50%"><center>图2：误差与模型复杂度的关系</center><p>可见欠拟合通常对应一个很大的偏差，而过拟合常对应一个很大的方差。</p><h2>六. 偏差，方差的处理</h2><p>偏差，方差的处理实际上就是对欠拟合和过拟合的处理，常用处理方式如下：偏差：避免欠拟合。</p><ul><li>寻找更好的特征 -- 具有代表性。</li><li>用更多的特征 -- 增大输入向量的维度。（增加模型复杂度）</li></ul><p>方差：避免过拟合</p><ul><li>增大数据集合 -- 使用更多的数据，减少数据扰动所造成的影响</li><li>减少数据特征 -- 减少数据维度，减少模型复杂度</li><li>正则化方法</li><li>交叉验证法</li></ul><h5>参考文献：</h5><blockquote><p>[1] segmentfault.chen_h：<a href="https://segmentfault.com/a/1190000012461409" target="_blank" rel="noopener">你真的理解机器学习中偏差 - 方差之间的权衡吗？</a><br>[2] 博客园.Terminator 2050：<a href="https://www.cnblogs.com/DicksonJYL/p/9485255.html" target="_blank" rel="noopener">【机器学习】理解方差、偏差且其泛化误差的关系</a><br>[3] 简书.X猪：<a href="https://www.jianshu.com/p/8c7f033be58a" target="_blank" rel="noopener">偏差(Bias)和方差(Variance)——机器学习中的模型选择</a><br>[4] 知乎.Microstrong：<a href="https://zhuanlan.zhihu.com/p/38853908" target="_blank" rel="noopener">偏差（Bias）与方差（Variance）</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对偏差-方差的权衡关系对于理解机器学习模型尤为重要，所谓学习模型是从给定的数据集中找到一个最佳的映射函数$f(x)$，它不仅在已知数据上有优异的表现，同时在未知数据上也要具有最佳预测能力。对于训练好的模型，我们难以直接判断它在未知数据上的预测情况，而偏差-方差的权衡给了我们一个判断的准则。对方差-偏差的理解，也有助于理解如何提高机器学习模型性能的深层机制。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>逻辑回归模型</title>
    <link href="http://yoursite.com/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-01-09T02:08:41.000Z</published>
    <updated>2020-01-09T02:37:46.657Z</updated>
    
    <content type="html"><![CDATA[<p>逻辑回归是一种用于二分类问题的机器学习模型，它是一种广义线性模型，与线性模型假设预测变量$y$服从高斯分布不同，逻辑回归假设预测变量服从伯努利分布。逻辑回归引入$sigmoid$函数，将预测值映射到$[0,1]$区间，表示样本属于某一类的可能性，由于$sigmoid$函数的引入，使得逻辑回归可以很轻松的处理二分类问题。</p><a id="more"></a><h2>Logistic分布</h2><h2>损失函数</h2><h2>梯度下降求解</h2><h2>正则化</h2><blockquote><p>[1] 阿泽<a href="https://zhuanlan.zhihu.com/p/74874291" target="_blank" rel="noopener">【机器学习】逻辑回归（非常详细）</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;逻辑回归是一种用于二分类问题的机器学习模型，它是一种广义线性模型，与线性模型假设预测变量$y$服从高斯分布不同，逻辑回归假设预测变量服从伯努利分布。逻辑回归引入$sigmoid$函数，将预测值映射到$[0,1]$区间，表示样本属于某一类的可能性，由于$sigmoid$函数的引入，使得逻辑回归可以很轻松的处理二分类问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>时间数据类型处理</title>
    <link href="http://yoursite.com/2019/12/24/%E6%97%B6%E9%97%B4%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2019/12/24/%E6%97%B6%E9%97%B4%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%A4%84%E7%90%86/</id>
    <published>2019-12-24T06:18:06.000Z</published>
    <updated>2020-01-09T02:19:12.446Z</updated>
    
    <content type="html"><![CDATA[<p>编写Python程序处理数据时，常遇到字符串类型的时间数据，如：“2019-12-27 00:27:27”。在数据分析时，会遇到涉及时间的加减计算，如“计算距当前日期30天后的日期？”，“今天是星期几？”，对字符串类型的进行这样的计算会比较繁琐，Python中的time模块和datetime模块提供了用来表达时间类型的数据对象。有时，我们也需要整型格式的时间戳来表示时间，以方便程序的处理，这时我们会面临这三种时间数据对象的转换。</p><a id="more"></a><p><img src="https://i.loli.net/2020/01/09/OTqAfpJQy6b9rxa.png" alt></p><h4>获取当前时间戳</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seconds = time.time()</span><br></pre></td></tr></table></figure><h4>时间戳格式转时间格式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time = time.localtime(seconds)</span><br></pre></td></tr></table></figure><h4>时间格式转字符串格式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>,time)</span><br></pre></td></tr></table></figure><h4>时间戳转换为字符串</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>, time.localtime(seconds))</span><br></pre></td></tr></table></figure><h4>字符串转换为时间戳</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time.mktime(time.strptime(<span class="string">"2018-08-07"</span>, <span class="string">"%Y-%m-%d"</span>))</span><br></pre></td></tr></table></figure><blockquote><p>[1] lwb444:<a href="https://www.cnblogs.com/linwenbin/p/10905341.html" target="_blank" rel="noopener">《Python 时间戳/字符串/时间 转换》</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;编写Python程序处理数据时，常遇到字符串类型的时间数据，如：“2019-12-27 00:27:27”。在数据分析时，会遇到涉及时间的加减计算，如“计算距当前日期30天后的日期？”，“今天是星期几？”，对字符串类型的进行这样的计算会比较繁琐，Python中的time模块和datetime模块提供了用来表达时间类型的数据对象。有时，我们也需要整型格式的时间戳来表示时间，以方便程序的处理，这时我们会面临这三种时间数据对象的转换。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>核主成分分析</title>
    <link href="http://yoursite.com/2019/10/24/%E6%A0%B8%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2019/10/24/%E6%A0%B8%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/</id>
    <published>2019-10-24T09:06:46.000Z</published>
    <updated>2020-01-08T18:01:25.767Z</updated>
    
    <content type="html"><![CDATA[<p>在核主成分分析中，我们认为原始数据集有更高的维数，我们可以在更高维的空间中做PCA分析（即在更高维空间里，把原始数据向不同的方向投影）。原因在于：在低维空间难以线性分类的数据点，我们有可能再更高维度上找到合适的高维线性分类平面。</p><a id="more"></a><h4>一. 核函数</h4><h5>1. 核函数的定义</h5><p>假设输入空间为X，特征空间为F（特征空间具有很高的维数，甚至无穷维），存在一个映射$\phi$将空间X中的点x映射到空间F中的点f：$$ \phi(x): X \rightarrow F $$实际应用中，我们难以确定映射$\phi$的具体形式，然而我们的主要计算需求是计算两个向量的内积，通过引入核函数，我们不需要知道映射的具体形式，便可通过低维空间的向量来计算高维空间向量的内积。<br>设：$x$，$z$是低维空间任意两个向量，那么核函数满足下述关系：$$ K(x,z) = \phi(x) \cdot \phi(z) $$</p><p>比如我们定义如下二维到三维的映射函数（这里，$x_i$表示样本$x$的第$i$个分量）：$$ \phi(x) = (x_1^2, \sqrt{2} x_1 x_2, x_2^2) $$可以验证：$$ K(x,z) = \phi(x) \cdot \phi(z) = (x \cdot z)^2 $$</p><h5>2. 核矩阵</h5><p>任意两个样本点映射到高维空间后的内积组成的矩阵称为核函数矩阵（这里，$x_i$表示第$i$个样本）：$$\begin{bmatrix}\phi(x_1) \cdot \phi(x_1) &amp; ... &amp; \phi(x_1) \cdot \phi(x_N) \\...                       &amp; ... &amp; ... \\\phi(x_N) \cdot \phi(x_1) &amp; ... &amp; \phi(x_N) \cdot \phi(x_N)\end{bmatrix}$$</p><p>常用的核函数的形式有如下几种：</p><h6>(1) 线性核</h6><p>$$ K(x,z) = x \cdot z $$</p><h6>(2) 多项式核</h6><p>$$ K(x,z) = (x \cdot z + 1)^r, r \in Z $$</p><h6>(3) 高斯核</h6><p>$$ K(x,z) = exp(-\frac{|x-z|^2}{2\sigma^2}), \sigma \in R, and\ \sigma \neq 0$$</p><p>采用核函数避免直接将原始向量映射到高维空间进行内积计算，一方面避开了映射函数的寻找，另一方面也减少了运算量。不过如果映射后的空间维数过高，导致模型在训练集上过拟合，在测试集上的泛化能力往往不加。</p><h4>二. 核技巧</h4><p>与线性主成分分析不同，核主成分分析通过在高维有映射后的特征空间进行主成分分析，设我们的原始数据矩阵为$X$：$$ [x_1, x_2, ..., x_N] $$$x_i$表示数据集中的第$i$个样本，是一个维度为$d$的列向量，这里矩阵的每一列为一个样本，样本数为$N$。我们通过映射函数$\phi(x)$将所有样本映射到维度为$D$的特征空间$F$，映射后的数据矩阵为$\phi(X)$：$$ [\phi(x_1), \phi(x_2), ..., \phi(x_N)] $$接下来对数据集$\phi(X)$进行PCA处理，这里预先假设$\phi(X)$已经经过中心化处理：$$ \sum_{i}^{N} \phi(x_i) = 0 $$在特征空间里，数据集$\phi(X)$的协方差矩阵为：$$ C_F = \frac{1}{N} \phi(X) \phi(X)^T = \frac{1}{N} \sum_{i=1}^N \phi(x_i) \phi(x_i)^T $$PCA计算实质是求解协方差矩阵$C_F$的本征值问题：$$ C_F p = \lambda p $$即：$$ \sum_{i=1}^N \phi(x_i) \phi(x_i)^T p = \lambda p $$这里不考虑因子$\frac{1}{N}$<br>降维时我们考虑的是本征值$\lambda \neq 0$的成分，我们对上式两边同消去$\lambda$，得到：$$ p = \sum_{i=1}^N \phi(x_i) [ \phi(x_i)^T p ] $$注意到上式方括号中的值是一个常数，我们用$\alpha_i$代替，于是：$$ p = \sum_{i=1}^N \alpha_i \phi(x_i) = \phi(X) \alpha $$$\alpha$是一个$N$维的向量：$[\alpha_1, \alpha_2, ..., \alpha_N]^T $<br>这里得到一个结论：特征值不为0的特征向量可表示为高维空间样本向量的线性组合。<br>将上式带入$C_F$的本征方程中得：$$ \phi(X)^T \phi(X) \phi(X)^T \phi(X) \alpha = \lambda \phi(X)^T \phi(X) \alpha $$定义核函数矩阵为：$K=\phi(X)^T \phi(X)$，上式化简为：$$ K \cdot K \alpha = \lambda K \alpha $$参考文献[1]证明为求解上式，则只需求解下述本征方程，该本征方程的特征值即原问题的本征值。$$ K \alpha = \lambda \alpha $$</p><blockquote><p>[1] B. Scholkopf, A. J. Smola, K. Muller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation 10 (5), 1299–1399, 1998.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在核主成分分析中，我们认为原始数据集有更高的维数，我们可以在更高维的空间中做PCA分析（即在更高维空间里，把原始数据向不同的方向投影）。原因在于：在低维空间难以线性分类的数据点，我们有可能再更高维度上找到合适的高维线性分类平面。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>主成分分析</title>
    <link href="http://yoursite.com/2019/10/24/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2019/10/24/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/</id>
    <published>2019-10-24T08:14:19.000Z</published>
    <updated>2020-01-08T18:01:18.383Z</updated>
    
    <content type="html"><![CDATA[<p>主成分分析是数据降维的重要手段，当样本中包含多个特征且特征之间强耦合时，将强耦合的特征综合成一个或几个特征，这样既减少了样本的维度，节约计算资源也减少过拟合的风险。在主成分分析中，每一个主成分都是数据集在某一个方向上的投影，不同方向上数据集的方差(variance)由该方向上的特征值(eigenvalue)决定，一般我们选择最大的几个特征值所在的特征向量(eigenvector)，认为在这些方向上，数据集包含了我们所需要的信息。</p><a id="more"></a><h4>1. 数据集标准化</h4><p>假设我们有一个数据集，每个样本有d个特征，共N个样本，我们构建一个$d \times N$维的矩阵$X = [x_1, x_2, ..., x_N]$，矩阵的每一列对应一个样本，矩阵的每一行对应一个特征。</p><p>由于数据集中的每个特征来源于不同的变量，往往具有不同的量纲，数量级上相差较大，通常需将数据集进行标准化处理，即尽可能让特征分布接近标准正态分布（均值为0，标准差为1）。$$ x' = \frac{x-\mu}{\sigma} $$$\mu$，$\sigma$分别是对应特征下的均值和标准差</p><h4>2. 协方差矩阵</h4><p>协方差矩阵是一个$d \times d$的方阵，是计算两两特征之间的协方差得到的，故是一个对称方阵。$$ C = E[(X-\mu)(X-\mu)^T] $$$\mu$是特征均值向量：$$\mu = E(X) $$</p><h4>3. 协方差矩阵的特征值与特征向量</h4><p>我们计算协方差矩阵的特征值和特征矢量，设d个特征矢量构成特征矩阵$U=[u_1, u_2, ..., u_d]$，d个特征值构成对角矩阵$\Lambda = diag(\lambda_1, \lambda_2, ..., \lambda_d)$，按照线性代数中矩阵特征矢量的定义有：$$ CU = U \Lambda $$于是：$$ C = U \Lambda U^T $$</p><h4>4. 方差贡献率</h4><p>每个主成分有与其对应的本征值，决定这该主成分的方差，方差越大表示该主成分包含的信息越多，方差越小，表示包含的信息越少（通常是数据噪声，但有时这部分成分也包含重要信息）。我们按照方差从大到小排序，选择前k个主成分（通常设定阈值90%，前k个主成分的累积方差贡献率不低于90%）。</p><h4>5. 数据降维</h4><p>选择对应的k个特征向量构成变换矩阵$U_k=[u_1, u_2, ..., u_k]$，将变换矩阵作用在原始数据集上得到降维后的数据集：$$ Y = U_k^T X $$</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;主成分分析是数据降维的重要手段，当样本中包含多个特征且特征之间强耦合时，将强耦合的特征综合成一个或几个特征，这样既减少了样本的维度，节约计算资源也减少过拟合的风险。在主成分分析中，每一个主成分都是数据集在某一个方向上的投影，不同方向上数据集的方差(variance)由该方向上的特征值(eigenvalue)决定，一般我们选择最大的几个特征值所在的特征向量(eigenvector)，认为在这些方向上，数据集包含了我们所需要的信息。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>特征工程</title>
    <link href="http://yoursite.com/2019/08/14/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E5%88%86%E7%AE%B1/"/>
    <id>http://yoursite.com/2019/08/14/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E5%88%86%E7%AE%B1/</id>
    <published>2019-08-14T08:54:07.000Z</published>
    <updated>2020-01-08T17:47:16.972Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>回归模型的评估指标</title>
    <link href="http://yoursite.com/2019/08/02/%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/"/>
    <id>http://yoursite.com/2019/08/02/%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/</id>
    <published>2019-08-02T06:47:01.000Z</published>
    <updated>2020-01-09T05:22:12.891Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习中回归模型的一些评估指标。</p><a id="more"></a><h4>解释方差（Explained Variance）</h4><p>$$ EV(y, \hat y) = 1 - \frac{Var\{y-\hat y\}}{Var\{y\}} $$</p><h4>均方误差（MSE）</h4><p>$$ MSE(y, \hat y) = \frac{1}{n} \sum_{i=1}^N (\hat y_i - y_i)^2  $$</p><h4>均方根误差（RMSE）</h4><p>$$ RMSE(y, \hat y) = \sqrt{\frac{1}{n} \sum_{i=1}^N (\hat y_i - y_i)^2}  $$</p><h4>平均绝对误差（MAE）</h4><p>$$ MAE(y, \hat y) = \frac{1}{N} \sum_{i=1}^N |\hat y_i - y_i| $$</p><h4>均值平方对数误差（MSLE）</h4><p>$$ MSLE(y, \hat y) = \frac{1}{N} \sum_{i=1}^N (ln(1+y_i)-ln(1+\hat y_i))^2 $$</p><h4>中位数绝对误差（MedianAE）</h4><p>$$ MedAE(y,\hat y) = median(|y_1 - \hat y_1|, ..., |y_n-\hat y_n|) $$</p><h4>R方（R Squared）</h4><p>$$ R^2 = 1 - \frac{\sum_{i=1}^N(y_i - \hat y_i)^2}{\sum_{i=1}^N(y_i - \bar y)^2 } $$结果在-1~1之间，如果结果为0，那么模型和猜测差不多，如果为1，说明模型没有误差，如果结果小于0，那么模型还不如随机猜测。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;机器学习中回归模型的一些评估指标。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>波士顿房价数据分析</title>
    <link href="http://yoursite.com/2019/07/26/%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2019/07/26/%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</id>
    <published>2019-07-26T08:09:46.000Z</published>
    <updated>2020-01-09T03:46:26.670Z</updated>
    
    <content type="html"><![CDATA[<p>波士顿房价集来源于1978年美国某经济学杂志。该数据集包含若干波士顿房屋的价格及其各项数据，每个数据项包含14个数据，分别是房屋均价及周边犯罪率、是否在河边等相关信息，其中最后一个数据是房屋均价。</p><a id="more"></a><p>数据集中有506条数据，13个特征，1个预测变量，数据集的具体情况下表简单说明。</p><table><thead><tr><th style="text-align:center">编号</th><th style="text-align:left">特征</th><th style="text-align:left">特征说明</th><th style="text-align:left">简单统计</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:left">CRIM</td><td style="text-align:left">城镇人均犯罪率</td><td style="text-align:left">最高：88.97；最低：0.0063；平均：3.6135；中位数：0.2565</td></tr><tr><td style="text-align:center">2</td><td style="text-align:left">ZN</td><td style="text-align:left">住宅用地所占比例</td><td style="text-align:left">最高：100；最低：0；平均：11.3636；中位数：0</td></tr><tr><td style="text-align:center">3</td><td style="text-align:left">INDUS</td><td style="text-align:left">城镇中非商业用地所占比例</td><td style="text-align:left">最高：27.74；最低：0.46；平均：11.13；中位数：9.69</td></tr><tr><td style="text-align:center">4</td><td style="text-align:left">CHAS</td><td style="text-align:left">CHAS查尔斯和虚拟变量</td><td style="text-align:left">只有两种取值：0和1，471个样本取值为0；35个样本取值为1</td></tr><tr><td style="text-align:center">5</td><td style="text-align:left">NOX</td><td style="text-align:left">环保指标(一氧化氮浓度)</td><td style="text-align:left">最高：0.8710；最低：0.3850；平均：0.5547；中位数：0.5380</td></tr><tr><td style="text-align:center">6</td><td style="text-align:left">RM</td><td style="text-align:left">每栋住宅的房间数</td><td style="text-align:left">最高：8.780；最低：3.561；平均：6.285；中位数：6.208</td></tr><tr><td style="text-align:center">7</td><td style="text-align:left">AGE</td><td style="text-align:left">1940年以前建成的自助单位比例</td><td style="text-align:left">最高：100；最低：2.9；平均：68.57；中位数：77.5</td></tr><tr><td style="text-align:center">8</td><td style="text-align:left">DIS</td><td style="text-align:left">距离五个波士顿就业中心的加权距离</td><td style="text-align:left">最高：12.1265；最低：1.1296；平均：3.7950；中位数：3.2074</td></tr><tr><td style="text-align:center">9</td><td style="text-align:left">RAD</td><td style="text-align:left">距离高速公路的便利指数</td><td style="text-align:left">最高：24.0；最低：1.0；平均：0.5494；中位数：5.0</td></tr><tr><td style="text-align:center">10</td><td style="text-align:left">TAX</td><td style="text-align:left">每一万美元的不动产税率</td><td style="text-align:left">最高：711.0；最低：187.0；平均：408.2；中位数：330</td></tr><tr><td style="text-align:center">11</td><td style="text-align:left">PTRATIO</td><td style="text-align:left">城镇中教师学生比例</td><td style="text-align:left">最高：22.0；最低：12.6；平均：18.45；中位数：19.05</td></tr><tr><td style="text-align:center">12</td><td style="text-align:left">B</td><td style="text-align:left">城镇中黑人比例</td><td style="text-align:left">最高：396.9；最低：0.32；平均：356.67；中位数：391.44</td></tr><tr><td style="text-align:center">13</td><td style="text-align:left">LSTAT</td><td style="text-align:left">人口中地位低下者的比例</td><td style="text-align:left">最高：37.97；最低：1.73；平均：12.65；中位数：11.36</td></tr><tr><td style="text-align:center">14</td><td style="text-align:left">MEDV</td><td style="text-align:left">平均房价</td><td style="text-align:left">最高：50.0；最低：5.0；平均：22.53；中位数：21.2</td></tr></tbody></table><p>其中有三个特征与预测变量的相关系数高于0.5，分别是：LSTAT，RM，PTRATIO，它们与预测变量的散点图如下：<img src="https://i.loli.net/2020/01/09/p2YljZRkX6hObBa.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;波士顿房价集来源于1978年美国某经济学杂志。该数据集包含若干波士顿房屋的价格及其各项数据，每个数据项包含14个数据，分别是房屋均价及周边犯罪率、是否在河边等相关信息，其中最后一个数据是房屋均价。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Data Analysis" scheme="http://yoursite.com/categories/Data-Analysis/"/>
    
    
  </entry>
  
  <entry>
    <title>NexT主题配置教程</title>
    <link href="http://yoursite.com/2019/07/25/NexT%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/"/>
    <id>http://yoursite.com/2019/07/25/NexT%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/</id>
    <published>2019-07-25T09:19:31.000Z</published>
    <updated>2019-07-25T09:19:31.094Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hexo博客搭建教程</title>
    <link href="http://yoursite.com/2019/07/25/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"/>
    <id>http://yoursite.com/2019/07/25/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/</id>
    <published>2019-07-25T09:19:15.000Z</published>
    <updated>2019-07-25T09:19:15.624Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>json格式文件的导入与导出</title>
    <link href="http://yoursite.com/2019/05/15/json%E6%A0%BC%E5%BC%8F%E6%96%87%E4%BB%B6%E7%9A%84%E5%AF%BC%E5%85%A5%E4%B8%8E%E5%AF%BC%E5%87%BA/"/>
    <id>http://yoursite.com/2019/05/15/json%E6%A0%BC%E5%BC%8F%E6%96%87%E4%BB%B6%E7%9A%84%E5%AF%BC%E5%85%A5%E4%B8%8E%E5%AF%BC%E5%87%BA/</id>
    <published>2019-05-15T06:38:23.000Z</published>
    <updated>2020-01-08T17:59:20.462Z</updated>
    
    <content type="html"><![CDATA[<p>JSON(JavaScript Object Notation)是一种轻量级的数据交换格式。它基于ECMAScript的一个子集。JSON采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯(包括C、C++、Java、JavaScript、Perl、Python 等)。这些特性使JSON成为理想的数据交换语言。易于人阅读和编写，同时也易于机器解析和生成(一般用于提升网络传输速率)。</p><a id="more"></a><p>在Python中有默认的模块实现json数据的导入导出。</p><ol><li><p>导入json模块</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></table></figure></li><li><p>将Python字典转换为字符串</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义一个python字典数据</span></span><br><span class="line">test_dict = &#123;<span class="string">'bigberg'</span>: [<span class="number">7600</span>, &#123;<span class="number">1</span>: [[<span class="string">'iPhone'</span>, <span class="number">6300</span>], [<span class="string">'Bike'</span>, <span class="number">800</span>], [<span class="string">'shirt'</span>, <span class="number">300</span>]]&#125;]&#125;</span><br><span class="line"><span class="comment"># 将字典转换为json字符串</span></span><br><span class="line">json_str = json.dumps(test_dict)</span><br></pre></td></tr></table></figure></li><li><p>将字符串转换为Python字典</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将json字符串还原为字典</span></span><br><span class="line">new_dict = json.loads(json_str)</span><br></pre></td></tr></table></figure></li><li><p>将Python字典写入json格式文件中</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将字典new_dict中的数据写入json格式文件中</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"record.json"</span>,<span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(new_dict,f)</span><br></pre></td></tr></table></figure></li><li><p>从json格式文件中导入数据到Python字典</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"record.json"</span>,<span class="string">'r'</span>) <span class="keyword">as</span> load_f:</span><br><span class="line">    load_dict = json.load(load_f)</span><br><span class="line">    print(load_dict)</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;JSON(JavaScript Object Notation)是一种轻量级的数据交换格式。它基于ECMAScript的一个子集。
JSON采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯(包括C、C++、Java、
JavaScript、Perl、Python 等)。这些特性使JSON成为理想的数据交换语言。易于人阅读和编写，同时也
易于机器解析和生成(一般用于提升网络传输速率)。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>线性回归模型</title>
    <link href="http://yoursite.com/2018/08/23/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/08/23/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</id>
    <published>2018-08-23T04:38:12.000Z</published>
    <updated>2020-01-09T15:49:52.722Z</updated>
    
    <content type="html"><![CDATA[<p>线性回归是最简单的机器学习模型，和逻辑回归都是广义线性模型的一种特殊形式。线性模型假设模型的预测误差满足正态分布，在极大似然原理的基础上可以导出线性回归模型的损失函数的形式为平方误差函数。由于线性回归模型简单，易于解释，被广泛用于各种回归分析中。虽然模型简单，但是涉及到机器学习模型的许多基础概念，对线性模型的学习有助于对机器学习一些基本概念的深入理解。</p><a id="more"></a><h2>一. 基本形式</h2><p>由$d$个属性描述的实例$x=(x_1; x_2; ...; x_d)$，线性回归试图学习一个通过属性的线性组合来进行预测的函数：$$ f(x)=w_1 x^1 + w_2 x^2 + ... + w_d x^d + b$$</p><!-- more --><p>我们可以用向量形式表达如下：$$ f(x)=w^T x+b $$其中：$ w =\begin{pmatrix}{ w_1 } \\{ w_2 } \\{ ... } \\{ w_d }\end{pmatrix}$是权重系数向量，b是模型的偏置。预测结果与实际值的差异称为残差：$$ \epsilon_i = y_i - \hat y_i $$线性归回有五个基本假设：</p><ul><li><p><strong>线性性与可加性</strong><br>线性性即$x^i$没变动一个单位，$f(x)$相应变动$w_i$个单位，与$x^i$的具体数值无关；可加性说明$x^i$对$f(x)$的影响独立与其它变量。</p></li><li><p><strong>残差项相互独立</strong><br>若不互相独立，那么模型具有自相关性，此时数据集认为是一个时序数据，需要时序模型去处理。</p></li><li><p><strong>自变量间相互独立</strong><br>若不满足，那么模型具有多重共线性。</p></li><li><p><strong>残差项方差为常数</strong><br>满足时，模型具有同方差性；不满足模型具有异方差性，异方差性常常出现在有异常值的数据集上。</p></li><li><p><strong>残差项满足正态分布</strong><br>$$f(\epsilon_i;\mu,\sigma) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\epsilon_i-\mu)^2}{2\sigma^2}}$$</p></li></ul><h2>二. 损失函数</h2><p>模型的参数$w, b$决定模型预测能力的好坏，模型在训练集中的预测值与实际值的差距是我们的建模误差，代价函数即定义为模型预测误差的平方和：$$ L(w, b) = \frac{1}{2m} \sum_{i=1}^{N}(f(x_i)-y_i)^2 $$</p><p>我们的目标是选择使代价函数最小的模型参数：</p><p>$$  w^*, b^* = \underset{w,b}{\arg \min}\ L(w,b) = \underset{w,b}{\arg \min}\ \frac{1}{2m} \sum_{i=1}^{N}(f(x_i)-y_i)^2 $$</p><h2>三. 梯度下降法</h2><p>$$\begin{cases}w = w - \alpha \frac{\partial}{\partial w} L(w,b) \\b = b - \alpha \frac{\partial}{\partial b} L(w,b)\end{cases}$$这里，$\alpha$ 是学习率，偏导数的计算公式如下：\begin{cases}\frac{\partial}{\partial b} L(w,b) = \frac{1}{N}\sum\limits_{i=1}^{N}(f(x_i)-y_i) \\\frac{\partial}{\partial w} L(w,b) = \frac{1}{N}\sum\limits_{i=1}^{N}((f(x_i)-y_i) x_i)\end{cases}</p><h2>四. 最小二乘法</h2><h2>五. 欠拟合与过拟合</h2><p>如果我们仅仅只是寻找模型的损失函数何时取得最小值（在训练数据上误差最小），这么这只是一个最优化问题；当我们同时要求模型在未知数据（测试数据）上误差也尽可能小时，这是便是一个学习问题。机器学习中我们很看中模型在未知数据上的泛化能力，泛化能力强的模型才是好的模型。</p><p>对于训练好的模型，如果模型在训练集上的预测误差较大，那么它在测试集上的误差也会较大，这通常是欠拟合导致的；如果模型在训练集上的表现良好，而在测试集上的误差不尽人意，此时模型过拟合了。从偏差（bias）与方差（variance）的角度来解释这种现象：欠拟合会导致较高的偏差，过拟合会导致较高的方差，所以模型需要在偏差与方差之间做出一个权衡。</p><p>发生欠拟合时，模型通常没有很好的捕捉到数据的特征，一般解决方法有：</p><ul><li>增加特征数目，如新特征，特征组合，高次特征（多项式特征）等。</li><li>考虑非线性模型，如核SVM等。</li><li>减小正则化参数</li><li>局部加权回归</li><li>Boosting</li></ul><p>过拟合的一般解决方法有：</p><ul><li>simpler model structure（合适模型）</li><li>regularization（正则化）</li><li>data augmentation（数据集扩增）</li><li>dropout（删除隐藏层结点个数）</li><li>Bootstrap/Bagging（封装）</li><li>ensemble（集成）</li><li>early stopping（提前终止迭代）</li><li>utilize invariance（利用不变性）</li><li>Bayesian（贝叶斯方法）</li></ul><p>在线性回归模型中，常用正则化类解决过拟合现象，正则化分为两种L1正则化和L2正则化，分别对应着两种线性回归：<strong>Lasso回归</strong>和<strong>Ridge回归</strong>。</p><p>有多种角度可以解释过拟合现象：</p><ul><li><p>经典的从偏置-方差的角度解释，“正则化可以增大偏差当时降低方差”；</p></li><li><p>PAC学习理论认为正则化降低了模型的拟合能力，使得模型结构变得简单，模型的VC维下降，从而模型的期望误差与风险误差的区别越小。</p></li><li><p>贝叶斯先验解释认为正则化相当于给模型的参数引入先验概率。在L1正则化中，认为模型的参数满足拉普拉斯分布；L2正则化中，认为模型的参数满足标准高斯分布。通过对贝叶斯后验概率最大化（最大后验估计），可以推导出带L1或L2正则项的损失函数。</p></li><li><p>奥卡姆剃刀原则“保证性能差别不大的情况下，越是简单的模型泛化性能越好”</p></li></ul><h5>参考文献：</h5><blockquote><p>[1] 知乎.小马：<a href="https://zhuanlan.zhihu.com/p/95714027" target="_blank" rel="noopener">机器学习-线性回归2-正则化</a><br>[2] 知乎.慢慢成长：<a href="https://zhuanlan.zhihu.com/p/29707029" target="_blank" rel="noopener">机器学习防止欠拟合、过拟合方法</a><br>[3] 知乎.十三：<a href="https://zhuanlan.zhihu.com/p/62457875" target="_blank" rel="noopener">线性回归中的正则化</a><br>[4] 知乎.bingo酱：<a href="https://zhuanlan.zhihu.com/p/35356992" target="_blank" rel="noopener">L1正则化与L2正则化</a><br>[5] CSDN.Noob_daniel：<a href="https://blog.csdn.net/Noob_daniel/article/details/76087829" target="_blank" rel="noopener">回归分析的五个基本假设</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;线性回归是最简单的机器学习模型，和逻辑回归都是广义线性模型的一种特殊形式。线性模型假设模型的预测误差满足正态分布，在极大似然原理的基础上可以导出线性回归模型的损失函数的形式为平方误差函数。由于线性回归模型简单，易于解释，被广泛用于各种回归分析中。虽然模型简单，但是涉及到机器学习模型的许多基础概念，对线性模型的学习有助于对机器学习一些基本概念的深入理解。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="线性回归" scheme="http://yoursite.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="梯度下降" scheme="http://yoursite.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
  <entry>
    <title>MNIST数据集</title>
    <link href="http://yoursite.com/2018/08/23/mnist%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%AF%BC%E5%85%A5/"/>
    <id>http://yoursite.com/2018/08/23/mnist%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%AF%BC%E5%85%A5/</id>
    <published>2018-08-23T02:21:00.000Z</published>
    <updated>2020-01-08T18:00:52.492Z</updated>
    
    <content type="html"><![CDATA[<p><strong>MNIST 数据集</strong>可在 <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a> 获取，它包含了四个部分：</p><a id="more"></a><ul><li><p><strong>Training set images：</strong></p><p>train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本)</p></li><li><p><strong>Training set labels：</strong></p><p>train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签)</p></li><li><p><strong>Test set images：</strong></p><p>t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本)</p></li><li><p><strong>Test set labels：</strong></p><p>t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签)</p></li></ul><p>MNIST 数据集来自美国国家标准与技术研究所，<strong>National Institute of Standards and Technology (NIST)</strong>，训练集 (training set) 由来自 250 个不同人手写的数字构成，其中 50% 是高中学生，50% 来自人口普查局 (the Census Bureau) 的工作人员。测试集(test set) 也是同样比例的手写数字数据。</p><p>图片是以字节的形式进行存储, 我们需要把它们读取到 NumPy array 中，以便训练和测试算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_mnist</span><span class="params">(path, kind=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Load MNIST data from `path`"""</span></span><br><span class="line">    labels_path = os.path.join(path,</span><br><span class="line">                               <span class="string">'%s-labels-idx1-ubyte'</span></span><br><span class="line">                               % kind)</span><br><span class="line">    images_path = os.path.join(path,</span><br><span class="line">                               <span class="string">'%s-images-idx3-ubyte'</span></span><br><span class="line">                               % kind)</span><br><span class="line">    <span class="keyword">with</span> open(labels_path, <span class="string">'rb'</span>) <span class="keyword">as</span> lbpath:</span><br><span class="line">        magic, n = struct.unpack(<span class="string">'&gt;II'</span>,</span><br><span class="line">                                 lbpath.read(<span class="number">8</span>))</span><br><span class="line">        labels = np.fromfile(lbpath,</span><br><span class="line">                             dtype=np.uint8)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(images_path, <span class="string">'rb'</span>) <span class="keyword">as</span> imgpath:</span><br><span class="line">        magic, num, rows, cols = struct.unpack(<span class="string">'&gt;IIII'</span>,</span><br><span class="line">                                               imgpath.read(<span class="number">16</span>))</span><br><span class="line">        images = np.fromfile(imgpath,</span><br><span class="line">                             dtype=np.uint8).reshape(len(labels), <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> images, labels</span><br></pre></td></tr></table></figure><p><code>load_mnist</code> 函数返回两个数组，第一个是一个 n x m 维的 NumPy array(<code>images</code>)，这里的 n 是样本数(行数)，m 是特征数(列数)。训练数据集包含 60000 个样本，测试数据集包含 10000 样本。在 MNIST 数据集中的每张图片由 28 x 28 个像素点构成，每个像素点用一个灰度值表示。在这里, 我们将 28 x 28 的像素展开为一个一维的行向量，这些行向量就是图片数组里的行(每行 784 个值，或者说每行就是代表了一张图片)。 <code>load_mnist</code> 函数返回的第二个数组(<code>labels</code>) 包含了相应的目标变量，也就是手写数字的类标签(整数 0-9)。</p><p>通过执行上面的代码，我们将会从刚刚解压 MNIST 数据集后的 mnist 目录下加载 60000 个训练样本和 10000 个测试样本。</p><p>为了了解 MNIST 中的图片看起来到底是个啥，让我们来对它们进行可视化处理。从 feature matrix 中将 784-像素值的向量 reshape 为之前的 28*28 的形状，然后通过 matplotlib 的 <code>imshow</code> 函数进行绘制：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(</span><br><span class="line">    nrows=<span class="number">2</span>,</span><br><span class="line">    ncols=<span class="number">5</span>,</span><br><span class="line">    sharex=<span class="keyword">True</span>,</span><br><span class="line">    sharey=<span class="keyword">True</span>, )</span><br><span class="line"></span><br><span class="line">ax = ax.flatten()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    img = X_train[y_train == i][<span class="number">0</span>].reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    ax[i].imshow(img, cmap=<span class="string">'Greys'</span>, interpolation=<span class="string">'nearest'</span>)</span><br><span class="line"></span><br><span class="line">ax[<span class="number">0</span>].set_xticks([])</span><br><span class="line">ax[<span class="number">0</span>].set_yticks([])</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>此外，我们还可以绘制某一数字的多个样本图片，来看一下这些手写样本到底有多不同：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(</span><br><span class="line">    nrows=<span class="number">5</span>,</span><br><span class="line">    ncols=<span class="number">5</span>,</span><br><span class="line">    sharex=<span class="keyword">True</span>,</span><br><span class="line">    sharey=<span class="keyword">True</span>, )</span><br><span class="line"></span><br><span class="line">ax = ax.flatten()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">25</span>):</span><br><span class="line">    img = X_train[y_train == <span class="number">7</span>][i].reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    ax[i].imshow(img, cmap=<span class="string">'Greys'</span>, interpolation=<span class="string">'nearest'</span>)</span><br><span class="line"></span><br><span class="line">ax[<span class="number">0</span>].set_xticks([])</span><br><span class="line">ax[<span class="number">0</span>].set_yticks([])</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;MNIST 数据集&lt;/strong&gt;可在 &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://yann.lecun.com/exdb/mnist/&lt;/a&gt; 获取，它包含了四个部分：&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>csv格式文件的导入与导出</title>
    <link href="http://yoursite.com/2018/08/22/csv%E6%A0%BC%E5%BC%8F%E6%96%87%E4%BB%B6%E7%9A%84%E5%AF%BC%E5%85%A5%E4%B8%8E%E5%AF%BC%E5%87%BA/"/>
    <id>http://yoursite.com/2018/08/22/csv%E6%A0%BC%E5%BC%8F%E6%96%87%E4%BB%B6%E7%9A%84%E5%AF%BC%E5%85%A5%E4%B8%8E%E5%AF%BC%E5%87%BA/</id>
    <published>2018-08-22T08:34:09.000Z</published>
    <updated>2020-01-08T17:58:55.471Z</updated>
    
    <content type="html"><![CDATA[<p>在数据处理中，常遇到csv格式的文件，下面简要介绍如何使用Python中的Pandas模块来读取csv文件中的数据。</p><a id="more"></a><h2>CSV文件格式</h2><p>CSV(Comma-Separated Values)文件以纯文本形式存储表格数据，文件由任意数目的记录组成，记录间以换行符分隔，每条记录由字段组成，字段间的分隔符可自定义，通常是逗号。下面的数据取自Kaggle中Titanic的乘客信息数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">PassengerId,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked</span><br><span class="line">892,3,&quot;Kelly, Mr. James&quot;,male,34.5,0,0,330911,7.8292,,Q</span><br><span class="line">893,3,&quot;Wilkes, Mrs. James (Ellen Needs)&quot;,female,47,1,0,363272,7,,S</span><br><span class="line">894,2,&quot;Myles, Mr. Thomas Francis&quot;,male,62,0,0,240276,9.6875,,Q</span><br><span class="line">895,3,&quot;Wirz, Mr. Albert&quot;,male,27,0,0,315154,8.6625,,S</span><br><span class="line">896,3,&quot;Hirvonen, Mrs. Alexander (Helga E Lindqvist)&quot;,female,22,1,1,3101298,12.2875,,S</span><br><span class="line">897,3,&quot;Svensson, Mr. Johan Cervin&quot;,male,14,0,0,7538,9.225,,S</span><br><span class="line">898,3,&quot;Connolly, Miss. Kate&quot;,female,30,0,0,330972,7.6292,,Q</span><br><span class="line">899,2,&quot;Caldwell, Mr. Albert Francis&quot;,male,26,1,1,248738,29,,S</span><br><span class="line">900,3,&quot;Abrahim, Mrs. Joseph (Sophie Halaut Easu)&quot;,female,18,0,0,2657,7.2292,,C</span><br><span class="line">901,3,&quot;Davies, Mr. John Samuel&quot;,male,21,2,0,A/4 48871,24.15,,S</span><br></pre></td></tr></table></figure><h2>Pandas模块</h2><ol><li><p>导入pandas模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure></li><li><p>读取csv文件中的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">"train.csv"</span>,sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure><p>参数<code>sep</code>设定csv文件中分隔符，默认为<code>,</code>。</p></li><li><p>数据集对应的参数名</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.columns</span><br></pre></td></tr></table></figure></li><li><p>第 $i$ 条记录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.iloc[i-1]</span><br></pre></td></tr></table></figure></li><li><p>数据集的大小</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.shape</span><br></pre></td></tr></table></figure></li><li><p>数据集的描述</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.describe</span><br></pre></td></tr></table></figure></li><li><p>数据集参数数组里的不同值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">titanic[<span class="string">'Sex'</span>].unique()</span><br></pre></td></tr></table></figure></li><li><p>数据集字符串到数字的映射</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">titanic.loc[titanic[<span class="string">'Sex'</span>]==<span class="string">'male'</span>,<span class="string">'Sex'</span>]=<span class="number">0</span></span><br><span class="line">titanic.loc[titanic[<span class="string">'Sex'</span>]==<span class="string">'female'</span>,<span class="string">'Sex'</span>]=<span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>数据集中缺失数据的补填</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">titanic[<span class="string">'Age'</span>]=titanic[<span class="string">'Age'</span>].fillna(titanic[<span class="string">'Age'</span>].median())</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在数据处理中，常遇到csv格式的文件，下面简要介绍如何使用Python中的Pandas模块来读取csv文件中的数据。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
  </entry>
  
</feed>
