<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据分箱提高分类模型性能]]></title>
    <url>%2F2019%2F08%2F14%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E5%88%86%E7%AE%B1%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[回归模型的评估指标]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[机器学习中回归模型的一些评估指标。 1. 解释方差（Explained Variance） $$ EV(y, \hat y) = 1 - \frac{Var\{y-\hat y\}}{Var\{y\}} $$ 2. 均方误差（MSE） $$ MSE(y, \hat y) = \frac{1}{n} \sum_{i=1}^N (\hat y_i - y_i)^2 $$ 3. 均方根误差（RMSE） $$ RMSE(y, \hat y) = \sqrt{\frac{1}{n} \sum_{i=1}^N (\hat y_i - y_i)^2} $$ 4. 平均绝对误差（MAE） $$ MAE(y, \hat y) = \frac{1}{N} \sum_{i=1}^N |\hat y_i - y_i| $$ 5. 均值平方对数误差（MSLE） $$ MSLE(y, \hat y) = \frac{1}{N} \sum_{i=1}^N (ln(1+y_i)-ln(1+\hat y_i))^2 $$ 6. 中位数绝对误差（MedianAE） $$ MedAE(y,\hat y) = median(|y_1 - \hat y_1|, ..., |y_n-\hat y_n|) $$ 7. R方（R Squared） $$ R^2 = 1 - \frac{\sum_{i=1}^N(y_i - \hat y_i)^2}{\sum_{i=1}^N(y_i - \bar y)^2 } $$ 结果在-1~1之间，如果结果为0，那么模型和猜测差不多，如果为1，说明模型没有误差，如果结果小于0，那么模型还不如随机猜测。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[波士顿房价数据集分析]]></title>
    <url>%2F2019%2F07%2F26%2F%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[波士顿房价集来源于1978年美国某经济学杂志。该数据集包含若干波士顿房屋的价格及其各项数据，每个数据项包含14个数据，分别是房屋均价及周边犯罪率、是否在河边等相关信息，其中最后一个数据是房屋均价。 1. 数据集中有506条数据，13个特征，1个标签，这是一个回归问题。 2. 各个特征字段的说明如下： 编号 字段 字段说明 1 CRIM 城镇人均犯罪率 2 ZN 住宅用地所占比例 3 INDUS 城镇中非商业用地所占比例 4 CHAS CHAS查尔斯和虚拟变量 5 NOX 环保指标(一氧化氮浓度) 6 RM 每栋住宅的房间数 7 AGE 1940年以前建成的自助单位比例 8 DIS 距离五个波士顿就业中心的加权距离 9 RAD 距离高速公路的便利指数 10 TAX 每一万美元的不动产税率 11 PTRATIO 城镇中教师学生比例 12 B 城镇中黑人比例 13 LSTAT 人口中地位低下者的比例 14 MEDV 平均房价 3. 各特征的取值分布情况 (1) CRIM (2) ZN (3) INDUS (4) CHAS (5) NOX (6) (7) (8) (9) (10) (11) (12) (13) (14) 4. 关联矩阵 关联矩阵]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NexT主题配置教程]]></title>
    <url>%2F2019%2F07%2F25%2FNexT%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客搭建教程]]></title>
    <url>%2F2019%2F07%2F25%2FHexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Python查询Solr]]></title>
    <url>%2F2019%2F07%2F24%2FPython%E6%9F%A5%E8%AF%A2Solr%2F</url>
    <content type="text"><![CDATA[Pysolr是基于Python的Apache Solr轻量级封装。它提供了服务器查询并返回基于查询的结果接口,提供了基本的查询，删除，更新功能。 solr请求中返回特定的字段类容 在web请求报文中按如下方式 1/?q=query&amp;fl=field1,field2,field3 Python面向对象机制 &quot;单下划线&quot; 开始的成员变量叫做保护变量，意思是只有类对象和子类对象自己能访问到这些变量 &quot;双下划线&quot; 开始的是私有成员，意思是只有类对象自己能访问，连子类对象也不能访问到这个数据 以双下划线开头和结尾的代表python里特殊方法专用的标识 _xxx 不能用 from moduleimport * 导入 __xxx 类中的私有变量名 __xxx__ 系统定义名字 核心风格：避免用下划线作为变量名的开始 python中并没有类似其他面向对象语言的private和public属性，无法在语言层面上用语言特性去封装数据。python用对属性和方法的命名约定来实现数据封装。 单下划线_开头的属性和方法属于类的私有成员，在模块或类外不可以使用。 双划线__开头的函数会导致访问名称变成其他形式，主要目的是为了将某个属性在子类中隐藏起来 Pysolr的基本机制 pysolr对web请求进一步封装，封装了对solr索引库的增删改查操作。这里，我们的主要需求是对solr索引库的查找操作，仅涉及到pysolr的查询命令，因此对pysolr的查询进一步封装，使得查询操作尽可能简单。 12345import pysolrsolr = pysolr.Solr(url)# 查询操作res = solr.search(params)]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene - Query Parser Syntax]]></title>
    <url>%2F2019%2F07%2F24%2FLucene%E6%9F%A5%E8%AF%A2%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Lucene是一个开放源代码的全文检索引擎工具包，它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎，由Apache软件基金会支持和提供。Lucene提供了一个简单却强大的应用程式接口，能够做全文索引和搜寻。Lucene最初是由Doug Cutting开发的，在SourceForge的网站上提供下载。在2001年9月作为高质量的开源Java产品加入到Apache软件基金会的Jakarta家族中。 Lucene提供一个查询解析器，可以将字符串解释成Lucene查询语句。 不同发行版的Lucene其查询解析器的语法稍有不同，以各版本为主。 不建议以程序生成查询字符串，查询字符串的设计初衷是方便人工输入查询，若需要程序生成查询字符串，建议使用Lucene的查询API。 一. 查询字符串 查询语句可分为词语（terms）和操作符（operators）,多个词语可以通过操作符形成更加复杂的搜索逻辑 二. 对单词（single terms）和语句（phrases）查询 12单词： &quot;hello&quot;, &quot;world&quot; 语句： &quot;hello world&quot; 三. 对某个字段进行搜索 12title:hello title:&quot;hello world&quot; Lucene中可定义默认字段，如默认字段是text，若匹配text字段下的go时，不需要指定字段名: 1title:hello AND go 它与 1title: hello AND text:go 是等价的。 四. 通过修饰符进行查找 可以在单个单词或者语句中添加通配符： ?匹配单个字符 *匹配0个或多个字符 注意：一些版本中，不支持将通配符放在搜索的开头 五. 模糊词查询（Fuzzy Searches） 123-&gt;想要搜索和test相近的词test~可以搜索出text或者tests等词 支持在~后面添加模糊系数，模糊系数[0-1]，越靠近1表示越相近,默认模糊系数为0.5。 1test~0.8 六. 邻近词查询（Proximity Searches） 前面的模糊词只是针对某个单词，在语句间也存在模糊搜索的概念，只不过不是单词的模糊，而是单词之间内容的模糊。 1234-&gt;想要搜索包含&quot;hello&quot;&quot;world&quot;的文档，这两个单词中间可以有一部分内容（这部分内容通过字符个数限制）&quot;hello world&quot;~10可以匹配&quot;hello 123 world&quot;或者&quot;hello,Tom,world&quot; 七. 范围查询（Range Searches） 支持范围搜索，可以指定最小值和最大值，会自动查找在这之间的文档。如果是单词，则会按照字典顺序搜索。 {}尖括号表示不包含最小值和最大值，可以单独使用 []方括号表示包含最小值和最大值，可以单独使用 12345-&gt;搜索成绩grade字段小于等于80分，大于60分的grade:&#123;60,80]-&gt;搜索名字在A和C之间的name:&#123;A,C&#125;返回，bone、baby、barry 八. 词语相关度查询（Boosting a Term） 如果单词的匹配度很高，一个文档中或者一个字段中可以匹配多次，那么可以提升该词的相关度。使用符号^提高相关度。 12=&gt;提高jarkarta的比重jakarta apache 可以使用下面语法： 1jakarta^4 apache 九. 布尔操作符（Boolean Operator） AND AND操作符用于连接两个搜索条件，仅当两个搜索条件都满足时，才认为匹配。通常用来做交集操作。也可以使用&amp;&amp;替换。注意必须使用大写。如果不使用AND，而是and，可能会被单做关键词进行搜索！ 1234-&gt;搜索同时包含tom和john的文档tom AND john或者tom &amp;&amp; john OR OR操作符用于连接两个搜索条件，当其中一个条件满足时，就认为匹配。通常用来做并集操作。也可以使用||替换。注意必须使用大写。 1234-&gt;搜索包含tom或者john的文档tom OR john或者tom || john NOT NOT操作符排除某个搜索条件。通常用来做差集操作也可以使用!替换。注意必须大写。 1234-&gt;搜索包含tom，不包含john的文档tom NOT john或者tom &amp;&amp; !john + 包含该操作符后跟着的搜索条件，作用和AND的差不多，但是支持单独使用如： 12-&gt;搜索包含tom的文档+tom - 排除该操作符后跟着的搜索条件，效果类似NOT，如： 12=&gt;搜索不包含tom的文档-tom 分组（Grouping） 支持使用小括号对每个子句进行分组，形成更为复杂的查询逻辑。 12-&gt;要搜索包含hello的文档中，也包含tom或者john的hello AND (tom OR john) 转义字符（Escaping Special Character） 由于Lucene中支持很多的符号，如： 1+ - &amp;&amp; || ! ( ) &#123; &#125; [ ] ^ &quot; ~ * ? : \ 因此如果需要搜索(1+1):2需要对改串进行转换，使用字符\ 1\(1\+1\)\:2]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solr使用教程]]></title>
    <url>%2F2019%2F07%2F17%2FSolr%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[能够检索相关信息是许多应用的基本要求，一些开发者和架构师通过 复杂的SQL请求来检索数据 在非关系型数据中进行检索 Solr可以作为一个搜索引擎使用 Apache Solr是一个流行的，快速的，开源的企业级搜索平台，应用在很多大型网站 中。 Apache Solr安装 有Java依赖，安装前先配置好java环境变量。 从Solr官网下载二进制安装包 解压包，进步bin目录 执行sudo bash ./install_solr_service.sh solr.zip -i /opt -d /var/solr -u solr -s solr -p 8983 安装脚本只认tgz和zip文件，若已经安装过solr，需加-f参数表示更新solr。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python读取导出json格式数据文件]]></title>
    <url>%2F2019%2F05%2F15%2FPython%E8%AF%BB%E5%8F%96%E5%AF%BC%E5%87%BAjson%E6%A0%BC%E5%BC%8F%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[JSON(JavaScript Object Notation)是一种轻量级的数据交换格式。它基于ECMAScript的一个子集。 JSON采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯(包括C、C++、Java、 JavaScript、Perl、Python 等)。这些特性使JSON成为理想的数据交换语言。易于人阅读和编写，同时也 易于机器解析和生成(一般用于提升网络传输速率)。 在Python中有默认的模块实现json数据的导入导出。 导入json模块 1import json 将Python字典转换为字符串 1234# 自定义一个python字典数据test_dict = &#123;'bigberg': [7600, &#123;1: [['iPhone', 6300], ['Bike', 800], ['shirt', 300]]&#125;]&#125;# 将字典转换为json字符串json_str = json.dumps(test_dict) 将字符串转换为Python字典 12# 将json字符串还原为字典new_dict = json.loads(json_str) 将Python字典写入json格式文件中 123# 将字典new_dict中的数据写入json格式文件中with open("record.json","w") as f: json.dump(new_dict,f) 从json格式文件中导入数据到Python字典 123with open("record.json",'r') as load_f: load_dict = json.load(load_f) print(load_dict)]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性模型]]></title>
    <url>%2F2018%2F08%2F23%2F%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[一. 基本形式 由$d$个属性描述的实例$x=(x_1; x_2; ...; x_d)$，线性回归试图学得一个通过属性的线性组合来预测的函数： $$ f(x)=w_1 x_1 + w_2 x_2 + ... + w_d x_d + b $$ 向量形式写成： $ f(x)=w^T x+b$ ，其中 $w = (w_1; w_2; ...; w_d)$ 二. 线性回归 设样本集的数目为$m$，$x$代表输入变量（特征），$y$代表输出变量（目标），$(x,y)$代表样本集中的实例，$(x^i,y^i)$代表样本集中第$i$个实例，函数$f(x)$是模型的假设。 1. 损失函数 模型的参数$w, b$决定模型预测能力的好坏，模型在训练集中的预测值与实际值的差距是我们的建模误差，代价函数即定义为建模误差的平方和： $ J(w, b) = \frac{1}{2m} \sum_{i=1}^{m}(f(x^i)-y^i)^2 $ 我们的目标是选择使代价函数最小的模型参数。 2. 梯度下降法 $w = w - \alpha \frac{\partial}{\partial w}J(w,b)$ $b = b - \alpha \frac{\partial}{\partial b}J(w,b)$ $\alpha$ 是学习率 $\frac{\partial}{\partial b}J(w,b) = \frac{1}{m}\sum_{i=1}^{m}(f(x^i)-y^i)$ $\frac{\partial}{\partial w}J(w,b) = \frac{1}{m}\sum_{i=1}^{m}((f(x^i)-y^i) x^i)$ 3. 最小二乘法 三. 逻辑回归 逻辑回归用来处理$y$值是离散情况的分类问题。 1. 分类问题 2. 假说表示 3. 判定边界 4. 代价函数与梯度下降 四. 正则化]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MNIST数据集]]></title>
    <url>%2F2018%2F08%2F23%2FMNIST%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[MNIST 数据集可在 http://yann.lecun.com/exdb/mnist/ 获取，它包含了四个部分： Training set images： train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本) Training set labels： train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签) Test set images： t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本) Test set labels： t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签) MNIST 数据集来自美国国家标准与技术研究所，National Institute of Standards and Technology (NIST)，训练集 (training set) 由来自 250 个不同人手写的数字构成，其中 50% 是高中学生，50% 来自人口普查局 (the Census Bureau) 的工作人员。测试集(test set) 也是同样比例的手写数字数据。 图片是以字节的形式进行存储, 我们需要把它们读取到 NumPy array 中，以便训练和测试算法。 12345678910111213141516171819202122232425import osimport structimport numpy as npdef load_mnist(path, kind='train'): """Load MNIST data from `path`""" labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind) images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind) with open(labels_path, 'rb') as lbpath: magic, n = struct.unpack('&gt;II', lbpath.read(8)) labels = np.fromfile(lbpath, dtype=np.uint8) with open(images_path, 'rb') as imgpath: magic, num, rows, cols = struct.unpack('&gt;IIII', imgpath.read(16)) images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784) return images, labels load_mnist 函数返回两个数组，第一个是一个 n x m 维的 NumPy array(images)，这里的 n 是样本数(行数)，m 是特征数(列数)。训练数据集包含 60000 个样本，测试数据集包含 10000 样本。在 MNIST 数据集中的每张图片由 28 x 28 个像素点构成，每个像素点用一个灰度值表示。在这里, 我们将 28 x 28 的像素展开为一个一维的行向量，这些行向量就是图片数组里的行(每行 784 个值，或者说每行就是代表了一张图片)。 load_mnist 函数返回的第二个数组(labels) 包含了相应的目标变量，也就是手写数字的类标签(整数 0-9)。 通过执行上面的代码，我们将会从刚刚解压 MNIST 数据集后的 mnist 目录下加载 60000 个训练样本和 10000 个测试样本。 为了了解 MNIST 中的图片看起来到底是个啥，让我们来对它们进行可视化处理。从 feature matrix 中将 784-像素值的向量 reshape 为之前的 28*28 的形状，然后通过 matplotlib 的 imshow 函数进行绘制： 1234567891011121314151617import matplotlib.pyplot as pltfig, ax = plt.subplots( nrows=2, ncols=5, sharex=True, sharey=True, )ax = ax.flatten()for i in range(10): img = X_train[y_train == i][0].reshape(28, 28) ax[i].imshow(img, cmap='Greys', interpolation='nearest')ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show() 此外，我们还可以绘制某一数字的多个样本图片，来看一下这些手写样本到底有多不同： 123456789101112131415fig, ax = plt.subplots( nrows=5, ncols=5, sharex=True, sharey=True, )ax = ax.flatten()for i in range(25): img = X_train[y_train == 7][i].reshape(28, 28) ax[i].imshow(img, cmap='Greys', interpolation='nearest')ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show()]]></content>
      <tags>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Pandas模块读取csv格式文件]]></title>
    <url>%2F2018%2F08%2F22%2F%E4%BD%BF%E7%94%A8Pandas%E6%A8%A1%E5%9D%97%E8%AF%BB%E5%8F%96csv%E6%A0%BC%E5%BC%8F%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[在数据处理中，常遇到csv格式的文件，下面简要介绍如何使用Python中的Pandas模块来 读取csv文件中的数据。 一. CSV文件 CSV(Comma-Separated Values)文件以纯文本形式存储表格数据，文件由任意数目的记录 组成，记录间以换行符分隔，每条记录由字段组成，字段间的分隔符可自定义，通常是 逗号。下面的数据取自Kaggle中Titanic的乘客信息数据。 1234567891011PassengerId,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked892,3,&quot;Kelly, Mr. James&quot;,male,34.5,0,0,330911,7.8292,,Q893,3,&quot;Wilkes, Mrs. James (Ellen Needs)&quot;,female,47,1,0,363272,7,,S894,2,&quot;Myles, Mr. Thomas Francis&quot;,male,62,0,0,240276,9.6875,,Q895,3,&quot;Wirz, Mr. Albert&quot;,male,27,0,0,315154,8.6625,,S896,3,&quot;Hirvonen, Mrs. Alexander (Helga E Lindqvist)&quot;,female,22,1,1,3101298,12.2875,,S897,3,&quot;Svensson, Mr. Johan Cervin&quot;,male,14,0,0,7538,9.225,,S898,3,&quot;Connolly, Miss. Kate&quot;,female,30,0,0,330972,7.6292,,Q899,2,&quot;Caldwell, Mr. Albert Francis&quot;,male,26,1,1,248738,29,,S900,3,&quot;Abrahim, Mrs. Joseph (Sophie Halaut Easu)&quot;,female,18,0,0,2657,7.2292,,C901,3,&quot;Davies, Mr. John Samuel&quot;,male,21,2,0,A/4 48871,24.15,,S 二. Pandas模块 导入pandas模块 1import pandas as pd 读取csv文件中的数据 1data = pd.read_csv("train.csv",sep=',') 参数sep设定csv文件中分隔符，默认为,。 数据集对应的参数名 1data.columns 第 $i$ 条记录 1data.iloc[i-1] 数据集的大小 1data.shape 数据集的描述 1data.describe 数据集参数数组里的不同值 1titanic['Sex'].unique() 数据集字符串到数字的映射 12titanic.loc[titanic['Sex']=='male','Sex']=0titanic.loc[titanic['Sex']=='female','Sex']=1 数据集中缺失数据的补填 1titanic['Age']=titanic['Age'].fillna(titanic['Age'].median())]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
