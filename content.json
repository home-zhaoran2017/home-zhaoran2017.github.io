{"meta":{"title":"Ran's Homepage","subtitle":null,"description":null,"author":"Ran Zhao","url":"http://yoursite.com"},"pages":[{"title":"about","date":"2019-04-02T16:03:44.000Z","updated":"2019-04-02T16:04:42.179Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"个人主页"},{"title":"categories","date":"2019-04-02T16:06:12.000Z","updated":"2019-04-02T16:06:12.528Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-11-17T11:47:09.000Z","updated":"2017-11-17T11:48:08.235Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Python查询Solr","slug":"Python查询Solr","date":"2019-07-24T11:59:55.000Z","updated":"2019-07-25T09:14:42.578Z","comments":true,"path":"2019/07/24/Python查询Solr/","link":"","permalink":"http://yoursite.com/2019/07/24/Python查询Solr/","excerpt":"Pysolr是基于Python的Apache Solr轻量级封装。它提供了服务器查询并返回基于查询的结果接口,提供了基本的查询，删除，更新功能。","text":"Pysolr是基于Python的Apache Solr轻量级封装。它提供了服务器查询并返回基于查询的结果接口,提供了基本的查询，删除，更新功能。 solr请求中返回特定的字段类容 在web请求报文中按如下方式 1/?q=query&amp;fl=field1,field2,field3 Python面向对象机制 “单下划线” 开始的成员变量叫做保护变量，意思是只有类对象和子类对象自己能访问到这些变量 “双下划线” 开始的是私有成员，意思是只有类对象自己能访问，连子类对象也不能访问到这个数据 以双下划线开头和结尾的代表python里特殊方法专用的标识 _xxx 不能用 from moduleimport * 导入 __xxx 类中的私有变量名 __xxx__ 系统定义名字 核心风格：避免用下划线作为变量名的开始 python中并没有类似其他面向对象语言的private和public属性，无法在语言层面上用语言特性去封装数据。python用对属性和方法的命名约定来实现数据封装。 单下划线_开头的属性和方法属于类的私有成员，在模块或类外不可以使用。 双划线__开头的函数会导致访问名称变成其他形式，主要目的是为了将某个属性在子类中隐藏起来 Pysolr的基本机制 pysolr对web请求进一步封装，封装了对solr索引库的增删改查操作。这里，我们的主要需求是对solr索引库的查找操作，仅涉及到pysolr的查询命令，因此对pysolr的查询进一步封装，使得查询操作尽可能简单。 12345import pysolrsolr = pysolr.Solr(url)# 查询操作res = solr.search(params)","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"}]},{"title":"Lucene - Query Parser Syntax","slug":"Lucene查询语法","date":"2019-07-24T11:58:56.000Z","updated":"2019-07-25T09:14:25.456Z","comments":true,"path":"2019/07/24/Lucene查询语法/","link":"","permalink":"http://yoursite.com/2019/07/24/Lucene查询语法/","excerpt":"Lucene是一个开放源代码的全文检索引擎工具包，它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎，由Apache软件基金会支持和提供。Lucene提供了一个简单却强大的应用程式接口，能够做全文索引和搜寻。Lucene最初是由Doug Cutting开发的，在SourceForge的网站上提供下载。在2001年9月作为高质量的开源Java产品加入到Apache软件基金会的Jakarta家族中。","text":"Lucene是一个开放源代码的全文检索引擎工具包，它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎，由Apache软件基金会支持和提供。Lucene提供了一个简单却强大的应用程式接口，能够做全文索引和搜寻。Lucene最初是由Doug Cutting开发的，在SourceForge的网站上提供下载。在2001年9月作为高质量的开源Java产品加入到Apache软件基金会的Jakarta家族中。 Lucene提供一个查询解析器，可以将字符串解释成Lucene查询语句。 不同发行版的Lucene其查询解析器的语法稍有不同，以各版本为主。 不建议以程序生成查询字符串，查询字符串的设计初衷是方便人工输入查询，若需要程序生成查询字符串，建议使用Lucene的查询API。 一. 查询字符串查询语句可分为词语（terms）和操作符（operators）,多个词语可以通过操作符形成更加复杂的搜索逻辑 二. 对单词（single terms）和语句（phrases）查询12单词： &quot;hello&quot;, &quot;world&quot; 语句： &quot;hello world&quot; 三. 对某个字段进行搜索12title:hello title:&quot;hello world&quot; Lucene中可定义默认字段，如默认字段是text，若匹配text字段下的go时，不需要指定字段名:1title:hello AND go 它与1title: hello AND text:go 是等价的。 四. 通过修饰符进行查找可以在单个单词或者语句中添加通配符： ?匹配单个字符 *匹配0个或多个字符注意：一些版本中，不支持将通配符放在搜索的开头 五. 模糊词查询（Fuzzy Searches）123-&gt;想要搜索和test相近的词test~可以搜索出text或者tests等词 支持在~后面添加模糊系数，模糊系数[0-1]，越靠近1表示越相近,默认模糊系数为0.5。1test~0.8 六. 邻近词查询（Proximity Searches）前面的模糊词只是针对某个单词，在语句间也存在模糊搜索的概念，只不过不是单词的模糊，而是单词之间内容的模糊。1234-&gt;想要搜索包含&quot;hello&quot;&quot;world&quot;的文档，这两个单词中间可以有一部分内容（这部分内容通过字符个数限制）&quot;hello world&quot;~10可以匹配&quot;hello 123 world&quot;或者&quot;hello,Tom,world&quot; 七. 范围查询（Range Searches）支持范围搜索，可以指定最小值和最大值，会自动查找在这之间的文档。如果是单词，则会按照字典顺序搜索。 {}尖括号表示不包含最小值和最大值，可以单独使用 []方括号表示包含最小值和最大值，可以单独使用12345-&gt;搜索成绩grade字段小于等于80分，大于60分的grade:&#123;60,80]-&gt;搜索名字在A和C之间的name:&#123;A,C&#125;返回，bone、baby、barry 八. 词语相关度查询（Boosting a Term）如果单词的匹配度很高，一个文档中或者一个字段中可以匹配多次，那么可以提升该词的相关度。使用符号^提高相关度。12=&gt;提高jarkarta的比重jakarta apache 可以使用下面语法：1jakarta^4 apache 九. 布尔操作符（Boolean Operator） AND AND操作符用于连接两个搜索条件，仅当两个搜索条件都满足时，才认为匹配。通常用来做交集操作。也可以使用&amp;&amp;替换。注意必须使用大写。如果不使用AND，而是and，可能会被单做关键词进行搜索！ 1234-&gt;搜索同时包含tom和john的文档tom AND john或者tom &amp;&amp; john OR OR操作符用于连接两个搜索条件，当其中一个条件满足时，就认为匹配。通常用来做并集操作。也可以使用||替换。注意必须使用大写。 1234-&gt;搜索包含tom或者john的文档tom OR john或者tom || john NOT NOT操作符排除某个搜索条件。通常用来做差集操作也可以使用!替换。注意必须大写。 1234-&gt;搜索包含tom，不包含john的文档tom NOT john或者tom &amp;&amp; !john + 包含该操作符后跟着的搜索条件，作用和AND的差不多，但是支持单独使用如： 12-&gt;搜索包含tom的文档+tom 排除该操作符后跟着的搜索条件，效果类似NOT，如： 12=&gt;搜索不包含tom的文档-tom 分组（Grouping） 支持使用小括号对每个子句进行分组，形成更为复杂的查询逻辑。 12-&gt;要搜索包含hello的文档中，也包含tom或者john的hello AND (tom OR john) 转义字符（Escaping Special Character） 由于Lucene中支持很多的符号，如： 1+ - &amp;&amp; || ! ( ) &#123; &#125; [ ] ^ &quot; ~ * ? : \\ 因此如果需要搜索(1+1):2需要对改串进行转换，使用字符\\ 1\\(1\\+1\\)\\:2","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"}]},{"title":"Solr使用教程","slug":"Solr使用教程","date":"2019-07-17T09:55:32.000Z","updated":"2019-07-25T09:14:17.130Z","comments":true,"path":"2019/07/17/Solr使用教程/","link":"","permalink":"http://yoursite.com/2019/07/17/Solr使用教程/","excerpt":"","text":"能够检索相关信息是许多应用的基本要求，一些开发者和架构师通过复杂的SQL请求来检索数据在非关系型数据中进行检索 Solr可以作为一个搜索引擎使用 Apache Solr是一个流行的，快速的，开源的企业级搜索平台，应用在很多大型网站中。 Apache Solr安装有Java依赖，安装前先配置好java环境变量。 从Solr官网下载二进制安装包 解压包，进步bin目录 执行sudo bash ./install_solr_service.sh solr.zip -i /opt -d /var/solr -u solr -s solr -p 8983安装脚本只认tgz和zip文件，若已经安装过solr，需加-f参数表示更新solr。","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"}]},{"title":"Python读取导出json格式数据文件","slug":"Python读取导出json格式数据文件","date":"2019-05-15T06:38:23.000Z","updated":"2019-07-25T09:14:05.790Z","comments":true,"path":"2019/05/15/Python读取导出json格式数据文件/","link":"","permalink":"http://yoursite.com/2019/05/15/Python读取导出json格式数据文件/","excerpt":"JSON(JavaScript Object Notation)是一种轻量级的数据交换格式。它基于ECMAScript的一个子集。JSON采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯(包括C、C++、Java、JavaScript、Perl、Python 等)。这些特性使JSON成为理想的数据交换语言。易于人阅读和编写，同时也易于机器解析和生成(一般用于提升网络传输速率)。","text":"JSON(JavaScript Object Notation)是一种轻量级的数据交换格式。它基于ECMAScript的一个子集。JSON采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯(包括C、C++、Java、JavaScript、Perl、Python 等)。这些特性使JSON成为理想的数据交换语言。易于人阅读和编写，同时也易于机器解析和生成(一般用于提升网络传输速率)。 在Python中有默认的模块实现json数据的导入导出。 导入json模块 1import json 将Python字典转换为字符串 1234# 自定义一个python字典数据test_dict = &#123;'bigberg': [7600, &#123;1: [['iPhone', 6300], ['Bike', 800], ['shirt', 300]]&#125;]&#125;# 将字典转换为json字符串json_str = json.dumps(test_dict) 将字符串转换为Python字典 12# 将json字符串还原为字典new_dict = json.loads(json_str) 将Python字典写入json格式文件中 123# 将字典new_dict中的数据写入json格式文件中with open(\"record.json\",\"w\") as f: json.dump(new_dict,f) 从json格式文件中导入数据到Python字典 123with open(\"record.json\",'r') as load_f: load_dict = json.load(load_f) print(load_dict)","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"线性模型","slug":"线性模型","date":"2018-08-23T04:38:12.000Z","updated":"2019-07-25T09:13:54.132Z","comments":true,"path":"2018/08/23/线性模型/","link":"","permalink":"http://yoursite.com/2018/08/23/线性模型/","excerpt":"一. 基本形式由$d$个属性描述的实例$x=(x_1; x_2; …; x_d)$，线性回归试图学得一个通过属性的线性组合来预测的函数： f(x)=w_1 x_1 + w_2 x_2 + ... + w_d x_d + b","text":"一. 基本形式由$d$个属性描述的实例$x=(x_1; x_2; …; x_d)$，线性回归试图学得一个通过属性的线性组合来预测的函数： f(x)=w_1 x_1 + w_2 x_2 + ... + w_d x_d + b 向量形式写成： $ f(x)=w^T x+b$ ，其中 $w = (w_1; w_2; …; w_d)$ 二. 线性回归设样本集的数目为$m$，$x$代表输入变量（特征），$y$代表输出变量（目标），$(x,y)$代表样本集中的实例，$(x^i,y^i)$代表样本集中第$i$个实例，函数$f(x)$是模型的假设。 1. 损失函数模型的参数$w, b$决定模型预测能力的好坏，模型在训练集中的预测值与实际值的差距是我们的建模误差，代价函数即定义为建模误差的平方和： $ J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m}(f(x^i)-y^i)^2 $ 我们的目标是选择使代价函数最小的模型参数。 2. 梯度下降法$w = w - \\alpha \\frac{\\partial}{\\partial w}J(w,b)$ $b = b - \\alpha \\frac{\\partial}{\\partial b}J(w,b)$ $\\alpha$ 是学习率 $\\frac{\\partial}{\\partial b}J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}(f(x^i)-y^i)$ $\\frac{\\partial}{\\partial w}J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}((f(x^i)-y^i) x^i)$ 3. 最小二乘法三. 逻辑回归逻辑回归用来处理$y$值是离散情况的分类问题。 1. 分类问题2. 假说表示3. 判定边界4. 代价函数与梯度下降四. 正则化","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"MNIST数据集","slug":"MNIST数据集","date":"2018-08-23T02:21:00.000Z","updated":"2019-07-25T09:13:15.952Z","comments":true,"path":"2018/08/23/MNIST数据集/","link":"","permalink":"http://yoursite.com/2018/08/23/MNIST数据集/","excerpt":"MNIST 数据集可在 http://yann.lecun.com/exdb/mnist/ 获取，它包含了四个部分：","text":"MNIST 数据集可在 http://yann.lecun.com/exdb/mnist/ 获取，它包含了四个部分： Training set images： train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本) Training set labels： train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签) Test set images： t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本) Test set labels： t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签) MNIST 数据集来自美国国家标准与技术研究所，National Institute of Standards and Technology (NIST)，训练集 (training set) 由来自 250 个不同人手写的数字构成，其中 50% 是高中学生，50% 来自人口普查局 (the Census Bureau) 的工作人员。测试集(test set) 也是同样比例的手写数字数据。 图片是以字节的形式进行存储, 我们需要把它们读取到 NumPy array 中，以便训练和测试算法。 12345678910111213141516171819202122232425import osimport structimport numpy as npdef load_mnist(path, kind='train'): \"\"\"Load MNIST data from `path`\"\"\" labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind) images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind) with open(labels_path, 'rb') as lbpath: magic, n = struct.unpack('&gt;II', lbpath.read(8)) labels = np.fromfile(lbpath, dtype=np.uint8) with open(images_path, 'rb') as imgpath: magic, num, rows, cols = struct.unpack('&gt;IIII', imgpath.read(16)) images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784) return images, labels load_mnist 函数返回两个数组，第一个是一个 n x m 维的 NumPy array(images)，这里的 n 是样本数(行数)，m 是特征数(列数)。训练数据集包含 60000 个样本，测试数据集包含 10000 样本。在 MNIST 数据集中的每张图片由 28 x 28 个像素点构成，每个像素点用一个灰度值表示。在这里, 我们将 28 x 28 的像素展开为一个一维的行向量，这些行向量就是图片数组里的行(每行 784 个值，或者说每行就是代表了一张图片)。 load_mnist 函数返回的第二个数组(labels) 包含了相应的目标变量，也就是手写数字的类标签(整数 0-9)。 通过执行上面的代码，我们将会从刚刚解压 MNIST 数据集后的 mnist 目录下加载 60000 个训练样本和 10000 个测试样本。 为了了解 MNIST 中的图片看起来到底是个啥，让我们来对它们进行可视化处理。从 feature matrix 中将 784-像素值的向量 reshape 为之前的 28*28 的形状，然后通过 matplotlib 的 imshow 函数进行绘制： 1234567891011121314151617import matplotlib.pyplot as pltfig, ax = plt.subplots( nrows=2, ncols=5, sharex=True, sharey=True, )ax = ax.flatten()for i in range(10): img = X_train[y_train == i][0].reshape(28, 28) ax[i].imshow(img, cmap='Greys', interpolation='nearest')ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show() 此外，我们还可以绘制某一数字的多个样本图片，来看一下这些手写样本到底有多不同： 123456789101112131415fig, ax = plt.subplots( nrows=5, ncols=5, sharex=True, sharey=True, )ax = ax.flatten()for i in range(25): img = X_train[y_train == 7][i].reshape(28, 28) ax[i].imshow(img, cmap='Greys', interpolation='nearest')ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show()","categories":[],"tags":[{"name":"数据集","slug":"数据集","permalink":"http://yoursite.com/tags/数据集/"}]},{"title":"使用Pandas模块读取csv格式文件","slug":"使用Pandas模块读取csv格式文件","date":"2018-08-22T08:34:09.000Z","updated":"2019-07-25T09:13:03.195Z","comments":true,"path":"2018/08/22/使用Pandas模块读取csv格式文件/","link":"","permalink":"http://yoursite.com/2018/08/22/使用Pandas模块读取csv格式文件/","excerpt":"在数据处理中，常遇到csv格式的文件，下面简要介绍如何使用Python中的Pandas模块来读取csv文件中的数据。","text":"在数据处理中，常遇到csv格式的文件，下面简要介绍如何使用Python中的Pandas模块来读取csv文件中的数据。 一. CSV文件CSV(Comma-Separated Values)文件以纯文本形式存储表格数据，文件由任意数目的记录组成，记录间以换行符分隔，每条记录由字段组成，字段间的分隔符可自定义，通常是逗号。下面的数据取自Kaggle中Titanic的乘客信息数据。 1234567891011PassengerId,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked892,3,&quot;Kelly, Mr. James&quot;,male,34.5,0,0,330911,7.8292,,Q893,3,&quot;Wilkes, Mrs. James (Ellen Needs)&quot;,female,47,1,0,363272,7,,S894,2,&quot;Myles, Mr. Thomas Francis&quot;,male,62,0,0,240276,9.6875,,Q895,3,&quot;Wirz, Mr. Albert&quot;,male,27,0,0,315154,8.6625,,S896,3,&quot;Hirvonen, Mrs. Alexander (Helga E Lindqvist)&quot;,female,22,1,1,3101298,12.2875,,S897,3,&quot;Svensson, Mr. Johan Cervin&quot;,male,14,0,0,7538,9.225,,S898,3,&quot;Connolly, Miss. Kate&quot;,female,30,0,0,330972,7.6292,,Q899,2,&quot;Caldwell, Mr. Albert Francis&quot;,male,26,1,1,248738,29,,S900,3,&quot;Abrahim, Mrs. Joseph (Sophie Halaut Easu)&quot;,female,18,0,0,2657,7.2292,,C901,3,&quot;Davies, Mr. John Samuel&quot;,male,21,2,0,A/4 48871,24.15,,S 二. Pandas模块 导入pandas模块 1import pandas as pd 读取csv文件中的数据 1data = pd.read_csv(\"train.csv\",sep=',') 参数sep设定csv文件中分隔符，默认为,。 数据集对应的参数名 1data.columns 第 $i$ 条记录 1data.iloc[i-1] 数据集的大小 1data.shape 数据集的描述 1data.describe 数据集参数数组里的不同值 1titanic['Sex'].unique() 数据集字符串到数字的映射 12titanic.loc[titanic['Sex']=='male','Sex']=0titanic.loc[titanic['Sex']=='female','Sex']=1 数据集中缺失数据的补填 1titanic['Age']=titanic['Age'].fillna(titanic['Age'].median())","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]}]}