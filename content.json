{"meta":{"title":"Ran's Homepage","subtitle":null,"description":null,"author":"Ran Zhao","url":"http://yoursite.com"},"pages":[{"title":"tags","date":"2017-11-17T11:47:09.000Z","updated":"2017-11-17T11:48:08.235Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"about","date":"2019-04-02T16:03:44.000Z","updated":"2019-04-02T16:04:42.179Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"个人主页"},{"title":"categories","date":"2019-04-02T16:06:12.000Z","updated":"2019-04-02T16:06:12.528Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Python 时间数据类型处理","slug":"Python-时间数据类型处理","date":"2019-12-24T06:18:06.000Z","updated":"2019-12-24T07:00:33.977Z","comments":true,"path":"2019/12/24/Python-时间数据类型处理/","link":"","permalink":"http://yoursite.com/2019/12/24/Python-时间数据类型处理/","excerpt":"编写Python程序处理数据时，常遇到时间格式的字符串数据，有时涉及到时间的加减运算。在Python中，有专门的时间结构的数据对象来表示时间数据，实际中，我们常喜欢用字符串数据来表示时间，或者整型的时间戳。因此常涉及这三种时间数据类型的转换操作。","text":"编写Python程序处理数据时，常遇到时间格式的字符串数据，有时涉及到时间的加减运算。在Python中，有专门的时间结构的数据对象来表示时间数据，实际中，我们常喜欢用字符串数据来表示时间，或者整型的时间戳。因此常涉及这三种时间数据类型的转换操作。 1. 获取当前时间戳 1seconds = time.time() 2. 时间戳格式转时间格式 1time = time.localtime(seconds) 3. 时间格式转字符串格式 1time.strftime(\"%Y-%m-%d %H:%M:%S\",time) 4. 时间戳转换为字符串 1time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(seconds)) 5. 字符串转换为时间戳 1time.mktime(time.strptime(\"2018-08-07\", \"%Y-%m-%d\")) [1] lwb444:《Python 时间戳/字符串/时间 转换》","categories":[],"tags":[]},{"title":"核主成分分析 Kernel PCA","slug":"核主成分分析-Kernel-PCA","date":"2019-10-24T09:06:46.000Z","updated":"2019-10-24T12:49:02.335Z","comments":true,"path":"2019/10/24/核主成分分析-Kernel-PCA/","link":"","permalink":"http://yoursite.com/2019/10/24/核主成分分析-Kernel-PCA/","excerpt":"在核主成分分析中，我们认为原始数据集有更高的维数，我们可以在更高维的空间中做PCA分析（即在更高维空间里，把原始数据向不同的方向投影）。原因在于：在低维空间难以线性分类的数据点，我们有可能再更高维度上找到合适的高维线性分类平面。","text":"在核主成分分析中，我们认为原始数据集有更高的维数，我们可以在更高维的空间中做PCA分析（即在更高维空间里，把原始数据向不同的方向投影）。原因在于：在低维空间难以线性分类的数据点，我们有可能再更高维度上找到合适的高维线性分类平面。 一. 核函数 1. 核函数的定义 假设输入空间为X，特征空间为F（特征空间具有很高的维数，甚至无穷维），存在一个映射$\\phi$将空间X中的点x映射到空间F中的点f： $$ \\phi(x): X \\rightarrow F $$ 实际应用中，我们难以确定映射$\\phi$的具体形式，然而我们的主要计算需求是计算两个向量的内积，通过引入核函数，我们不需要知道映射的具体形式，便可通过低维空间的向量来计算高维空间向量的内积。 设：$x$，$z$是低维空间任意两个向量，那么核函数满足下述关系： $$ K(x,z) = \\phi(x) \\cdot \\phi(z) $$ 比如我们定义如下二维到三维的映射函数（这里，$x_i$表示样本$x$的第$i$个分量）： $$ \\phi(x) = (x_1^2, \\sqrt{2} x_1 x_2, x_2^2) $$ 可以验证： $$ K(x,z) = \\phi(x) \\cdot \\phi(z) = (x \\cdot z)^2 $$ 2. 核矩阵 任意两个样本点映射到高维空间后的内积组成的矩阵称为核函数矩阵（这里，$x_i$表示第$i$个样本）： $$ \\begin{bmatrix} \\phi(x_1) \\cdot \\phi(x_1) &amp; ... &amp; \\phi(x_1) \\cdot \\phi(x_N) \\\\ ... &amp; ... &amp; ... \\\\ \\phi(x_N) \\cdot \\phi(x_1) &amp; ... &amp; \\phi(x_N) \\cdot \\phi(x_N) \\end{bmatrix} $$ 常用的核函数的形式有如下几种： (1) 线性核 $$ K(x,z) = x \\cdot z $$ (2) 多项式核 $$ K(x,z) = (x \\cdot z + 1)^r, r \\in Z $$ (3) 高斯核 $$ K(x,z) = exp(-\\frac{|x-z|^2}{2\\sigma^2}), \\sigma \\in R, and\\ \\sigma \\neq 0$$ 采用核函数避免直接将原始向量映射到高维空间进行内积计算，一方面避开了映射函数的寻找，另一方面也减少了运算量。不过如果映射后的空间维数过高，导致模型在训练集上过拟合，在测试集上的泛化能力往往不加。 二. 核技巧 与线性主成分分析不同，核主成分分析通过在高维有映射后的特征空间进行主成分分析，设我们的原始数据矩阵为$X$： $$ [x_1, x_2, ..., x_N] $$ $x_i$表示数据集中的第$i$个样本，是一个维度为$d$的列向量，这里矩阵的每一列为一个样本，样本数为$N$。 我们通过映射函数$\\phi(x)$将所有样本映射到维度为$D$的特征空间$F$，映射后的数据矩阵为$\\phi(X)$： $$ [\\phi(x_1), \\phi(x_2), ..., \\phi(x_N)] $$ 接下来对数据集$\\phi(X)$进行PCA处理，这里预先假设$\\phi(X)$已经经过中心化处理： $$ \\sum_{i}^{N} \\phi(x_i) = 0 $$ 在特征空间里，数据集$\\phi(X)$的协方差矩阵为： $$ C_F = \\frac{1}{N} \\phi(X) \\phi(X)^T = \\frac{1}{N} \\sum_{i=1}^N \\phi(x_i) \\phi(x_i)^T $$ PCA计算实质是求解协方差矩阵$C_F$的本征值问题： $$ C_F p = \\lambda p $$ 即： $$ \\sum_{i=1}^N \\phi(x_i) \\phi(x_i)^T p = \\lambda p $$ 这里不考虑因子$\\frac{1}{N}$ 降维时我们考虑的是本征值$\\lambda \\neq 0$的成分，我们对上式两边同消去$\\lambda$，得到： $$ p = \\sum_{i=1}^N \\phi(x_i) [ \\phi(x_i)^T p ] $$ 注意到上式方括号中的值是一个常数，我们用$\\alpha_i$代替，于是： $$ p = \\sum_{i=1}^N \\alpha_i \\phi(x_i) = \\phi(X) \\alpha $$ $\\alpha$是一个$N$维的向量：$[\\alpha_1, \\alpha_2, ..., \\alpha_N]^T $ 这里得到一个结论：特征值不为0的特征向量可表示为高维空间样本向量的线性组合。 将上式带入$C_F$的本征方程中得： $$ \\phi(X)^T \\phi(X) \\phi(X)^T \\phi(X) \\alpha = \\lambda \\phi(X)^T \\phi(X) \\alpha $$ 定义核函数矩阵为：$K=\\phi(X)^T \\phi(X)$，上式化简为： $$ K \\cdot K \\alpha = \\lambda K \\alpha $$ 参考文献[1]证明为求解上式，则只需求解下述本征方程，该本征方程的特征值即原问题的本征值。 $$ K \\alpha = \\lambda \\alpha $$ [1] B. Scholkopf, A. J. Smola, K. Muller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation 10 (5), 1299–1399, 1998.","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"主成分分析PCA","slug":"主成分分析PCA","date":"2019-10-24T08:14:19.000Z","updated":"2019-10-24T08:52:35.815Z","comments":true,"path":"2019/10/24/主成分分析PCA/","link":"","permalink":"http://yoursite.com/2019/10/24/主成分分析PCA/","excerpt":"主成分分析是数据降维的重要手段，当样本中包含多个特征且特征之间强耦合时，将强耦合的特征综合成一个或几个特征，这样既减少了样本的维度，节约计算资源也减少过拟合的风险。在主成分分析中，每一个主成分都是数据集在某一个方向上的投影，不同方向上数据集的方差(variance)由该方向上的特征值(eigenvalue)决定，一般我们选择最大的几个特征值所在的特征向量(eigenvector)，认为在这些方向上，数据集包含了我们所需要的信息。","text":"主成分分析是数据降维的重要手段，当样本中包含多个特征且特征之间强耦合时，将强耦合的特征综合成一个或几个特征，这样既减少了样本的维度，节约计算资源也减少过拟合的风险。在主成分分析中，每一个主成分都是数据集在某一个方向上的投影，不同方向上数据集的方差(variance)由该方向上的特征值(eigenvalue)决定，一般我们选择最大的几个特征值所在的特征向量(eigenvector)，认为在这些方向上，数据集包含了我们所需要的信息。 1. 数据集标准化 假设我们有一个数据集，每个样本有d个特征，共N个样本，我们构建一个$d \\times N$维的矩阵$X = [x_1, x_2, ..., x_N]$，矩阵的每一列对应一个样本，矩阵的每一行对应一个特征。 由于数据集中的每个特征来源于不同的变量，往往具有不同的量纲，数量级上相差较大，通常需将数据集进行标准化处理，即尽可能让特征分布接近标准正态分布（均值为0，标准差为1）。 $$ x' = \\frac{x-\\mu}{\\sigma} $$ $\\mu$，$\\sigma$分别是对应特征下的均值和标准差 2. 协方差矩阵 协方差矩阵是一个$d \\times d$的方阵，是计算两两特征之间的协方差得到的，故是一个对称方阵。 $$ C = E[(X-\\mu)(X-\\mu)^T] $$ $\\mu$是特征均值向量： $$\\mu = E(X) $$ 3. 协方差矩阵的特征值与特征向量 我们计算协方差矩阵的特征值和特征矢量，设d个特征矢量构成特征矩阵$U=[u_1, u_2, ..., u_d]$，d个特征值构成对角矩阵$\\Lambda = diag(\\lambda_1, \\lambda_2, ..., \\lambda_d)$，按照线性代数中矩阵特征矢量的定义有： $$ CU = U \\Lambda $$ 于是： $$ C = U \\Lambda U^T $$ 4. 方差贡献率 每个主成分有与其对应的本征值，决定这该主成分的方差，方差越大表示该主成分包含的信息越多，方差越小，表示包含的信息越少（通常是数据噪声，但有时这部分成分也包含重要信息）。我们按照方差从大到小排序，选择前k个主成分（通常设定阈值90%，前k个主成分的累积方差贡献率不低于90%）。 5. 数据降维 选择对应的k个特征向量构成变换矩阵$U_k=[u_1, u_2, ..., u_k]$，将变换矩阵作用在原始数据集上得到降维后的数据集： $$ Y = U_k^T X $$","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"数据分箱提高分类模型性能","slug":"特征工程之分箱","date":"2019-08-14T08:54:07.000Z","updated":"2019-08-14T08:54:07.596Z","comments":true,"path":"2019/08/14/特征工程之分箱/","link":"","permalink":"http://yoursite.com/2019/08/14/特征工程之分箱/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"回归模型的评估指标","slug":"回归模型的评估指标","date":"2019-08-02T06:47:01.000Z","updated":"2019-08-02T07:14:55.833Z","comments":true,"path":"2019/08/02/回归模型的评估指标/","link":"","permalink":"http://yoursite.com/2019/08/02/回归模型的评估指标/","excerpt":"机器学习中回归模型的一些评估指标。","text":"机器学习中回归模型的一些评估指标。 1. 解释方差（Explained Variance） $$ EV(y, \\hat y) = 1 - \\frac{Var\\{y-\\hat y\\}}{Var\\{y\\}} $$ 2. 均方误差（MSE） $$ MSE(y, \\hat y) = \\frac{1}{n} \\sum_{i=1}^N (\\hat y_i - y_i)^2 $$ 3. 均方根误差（RMSE） $$ RMSE(y, \\hat y) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^N (\\hat y_i - y_i)^2} $$ 4. 平均绝对误差（MAE） $$ MAE(y, \\hat y) = \\frac{1}{N} \\sum_{i=1}^N |\\hat y_i - y_i| $$ 5. 均值平方对数误差（MSLE） $$ MSLE(y, \\hat y) = \\frac{1}{N} \\sum_{i=1}^N (ln(1+y_i)-ln(1+\\hat y_i))^2 $$ 6. 中位数绝对误差（MedianAE） $$ MedAE(y,\\hat y) = median(|y_1 - \\hat y_1|, ..., |y_n-\\hat y_n|) $$ 7. R方（R Squared） $$ R^2 = 1 - \\frac{\\sum_{i=1}^N(y_i - \\hat y_i)^2}{\\sum_{i=1}^N(y_i - \\bar y)^2 } $$ 结果在-1~1之间，如果结果为0，那么模型和猜测差不多，如果为1，说明模型没有误差，如果结果小于0，那么模型还不如随机猜测。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"波士顿房价数据集分析","slug":"波士顿房价数据集分析","date":"2019-07-26T08:09:46.000Z","updated":"2019-07-31T01:42:27.340Z","comments":true,"path":"2019/07/26/波士顿房价数据集分析/","link":"","permalink":"http://yoursite.com/2019/07/26/波士顿房价数据集分析/","excerpt":"波士顿房价集来源于1978年美国某经济学杂志。该数据集包含若干波士顿房屋的价格及其各项数据，每个数据项包含14个数据，分别是房屋均价及周边犯罪率、是否在河边等相关信息，其中最后一个数据是房屋均价。","text":"波士顿房价集来源于1978年美国某经济学杂志。该数据集包含若干波士顿房屋的价格及其各项数据，每个数据项包含14个数据，分别是房屋均价及周边犯罪率、是否在河边等相关信息，其中最后一个数据是房屋均价。 1. 数据集中有506条数据，13个特征，1个标签，这是一个回归问题。 2. 各个特征字段的说明如下： 编号 字段 字段说明 1 CRIM 城镇人均犯罪率 2 ZN 住宅用地所占比例 3 INDUS 城镇中非商业用地所占比例 4 CHAS CHAS查尔斯和虚拟变量 5 NOX 环保指标(一氧化氮浓度) 6 RM 每栋住宅的房间数 7 AGE 1940年以前建成的自助单位比例 8 DIS 距离五个波士顿就业中心的加权距离 9 RAD 距离高速公路的便利指数 10 TAX 每一万美元的不动产税率 11 PTRATIO 城镇中教师学生比例 12 B 城镇中黑人比例 13 LSTAT 人口中地位低下者的比例 14 MEDV 平均房价 3. 各特征的取值分布情况 (1) CRIM (2) ZN (3) INDUS (4) CHAS (5) NOX (6) (7) (8) (9) (10) (11) (12) (13) (14) 4. 关联矩阵 关联矩阵","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"NexT主题配置教程","slug":"NexT主题配置教程","date":"2019-07-25T09:19:31.000Z","updated":"2019-07-25T09:19:31.094Z","comments":true,"path":"2019/07/25/NexT主题配置教程/","link":"","permalink":"http://yoursite.com/2019/07/25/NexT主题配置教程/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hexo博客搭建教程","slug":"Hexo博客搭建教程","date":"2019-07-25T09:19:15.000Z","updated":"2019-07-25T09:19:15.624Z","comments":true,"path":"2019/07/25/Hexo博客搭建教程/","link":"","permalink":"http://yoursite.com/2019/07/25/Hexo博客搭建教程/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Python查询Solr","slug":"Python查询Solr","date":"2019-07-24T11:59:55.000Z","updated":"2019-07-25T09:14:42.578Z","comments":true,"path":"2019/07/24/Python查询Solr/","link":"","permalink":"http://yoursite.com/2019/07/24/Python查询Solr/","excerpt":"Pysolr是基于Python的Apache Solr轻量级封装。它提供了服务器查询并返回基于查询的结果接口,提供了基本的查询，删除，更新功能。","text":"Pysolr是基于Python的Apache Solr轻量级封装。它提供了服务器查询并返回基于查询的结果接口,提供了基本的查询，删除，更新功能。 solr请求中返回特定的字段类容 在web请求报文中按如下方式 1/?q=query&amp;fl=field1,field2,field3 Python面向对象机制 &quot;单下划线&quot; 开始的成员变量叫做保护变量，意思是只有类对象和子类对象自己能访问到这些变量 &quot;双下划线&quot; 开始的是私有成员，意思是只有类对象自己能访问，连子类对象也不能访问到这个数据 以双下划线开头和结尾的代表python里特殊方法专用的标识 _xxx 不能用 from moduleimport * 导入 __xxx 类中的私有变量名 __xxx__ 系统定义名字 核心风格：避免用下划线作为变量名的开始 python中并没有类似其他面向对象语言的private和public属性，无法在语言层面上用语言特性去封装数据。python用对属性和方法的命名约定来实现数据封装。 单下划线_开头的属性和方法属于类的私有成员，在模块或类外不可以使用。 双划线__开头的函数会导致访问名称变成其他形式，主要目的是为了将某个属性在子类中隐藏起来 Pysolr的基本机制 pysolr对web请求进一步封装，封装了对solr索引库的增删改查操作。这里，我们的主要需求是对solr索引库的查找操作，仅涉及到pysolr的查询命令，因此对pysolr的查询进一步封装，使得查询操作尽可能简单。 12345import pysolrsolr = pysolr.Solr(url)# 查询操作res = solr.search(params)","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"}]},{"title":"Lucene - Query Parser Syntax","slug":"Lucene查询语法","date":"2019-07-24T11:58:56.000Z","updated":"2019-07-25T09:14:25.456Z","comments":true,"path":"2019/07/24/Lucene查询语法/","link":"","permalink":"http://yoursite.com/2019/07/24/Lucene查询语法/","excerpt":"Lucene是一个开放源代码的全文检索引擎工具包，它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎，由Apache软件基金会支持和提供。Lucene提供了一个简单却强大的应用程式接口，能够做全文索引和搜寻。Lucene最初是由Doug Cutting开发的，在SourceForge的网站上提供下载。在2001年9月作为高质量的开源Java产品加入到Apache软件基金会的Jakarta家族中。","text":"Lucene是一个开放源代码的全文检索引擎工具包，它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎，由Apache软件基金会支持和提供。Lucene提供了一个简单却强大的应用程式接口，能够做全文索引和搜寻。Lucene最初是由Doug Cutting开发的，在SourceForge的网站上提供下载。在2001年9月作为高质量的开源Java产品加入到Apache软件基金会的Jakarta家族中。 Lucene提供一个查询解析器，可以将字符串解释成Lucene查询语句。 不同发行版的Lucene其查询解析器的语法稍有不同，以各版本为主。 不建议以程序生成查询字符串，查询字符串的设计初衷是方便人工输入查询，若需要程序生成查询字符串，建议使用Lucene的查询API。 一. 查询字符串 查询语句可分为词语（terms）和操作符（operators）,多个词语可以通过操作符形成更加复杂的搜索逻辑 二. 对单词（single terms）和语句（phrases）查询 12单词： &quot;hello&quot;, &quot;world&quot; 语句： &quot;hello world&quot; 三. 对某个字段进行搜索 12title:hello title:&quot;hello world&quot; Lucene中可定义默认字段，如默认字段是text，若匹配text字段下的go时，不需要指定字段名: 1title:hello AND go 它与 1title: hello AND text:go 是等价的。 四. 通过修饰符进行查找 可以在单个单词或者语句中添加通配符： ?匹配单个字符 *匹配0个或多个字符 注意：一些版本中，不支持将通配符放在搜索的开头 五. 模糊词查询（Fuzzy Searches） 123-&gt;想要搜索和test相近的词test~可以搜索出text或者tests等词 支持在~后面添加模糊系数，模糊系数[0-1]，越靠近1表示越相近,默认模糊系数为0.5。 1test~0.8 六. 邻近词查询（Proximity Searches） 前面的模糊词只是针对某个单词，在语句间也存在模糊搜索的概念，只不过不是单词的模糊，而是单词之间内容的模糊。 1234-&gt;想要搜索包含&quot;hello&quot;&quot;world&quot;的文档，这两个单词中间可以有一部分内容（这部分内容通过字符个数限制）&quot;hello world&quot;~10可以匹配&quot;hello 123 world&quot;或者&quot;hello,Tom,world&quot; 七. 范围查询（Range Searches） 支持范围搜索，可以指定最小值和最大值，会自动查找在这之间的文档。如果是单词，则会按照字典顺序搜索。 {}尖括号表示不包含最小值和最大值，可以单独使用 []方括号表示包含最小值和最大值，可以单独使用 12345-&gt;搜索成绩grade字段小于等于80分，大于60分的grade:&#123;60,80]-&gt;搜索名字在A和C之间的name:&#123;A,C&#125;返回，bone、baby、barry 八. 词语相关度查询（Boosting a Term） 如果单词的匹配度很高，一个文档中或者一个字段中可以匹配多次，那么可以提升该词的相关度。使用符号^提高相关度。 12=&gt;提高jarkarta的比重jakarta apache 可以使用下面语法： 1jakarta^4 apache 九. 布尔操作符（Boolean Operator） AND AND操作符用于连接两个搜索条件，仅当两个搜索条件都满足时，才认为匹配。通常用来做交集操作。也可以使用&amp;&amp;替换。注意必须使用大写。如果不使用AND，而是and，可能会被单做关键词进行搜索！ 1234-&gt;搜索同时包含tom和john的文档tom AND john或者tom &amp;&amp; john OR OR操作符用于连接两个搜索条件，当其中一个条件满足时，就认为匹配。通常用来做并集操作。也可以使用||替换。注意必须使用大写。 1234-&gt;搜索包含tom或者john的文档tom OR john或者tom || john NOT NOT操作符排除某个搜索条件。通常用来做差集操作也可以使用!替换。注意必须大写。 1234-&gt;搜索包含tom，不包含john的文档tom NOT john或者tom &amp;&amp; !john + 包含该操作符后跟着的搜索条件，作用和AND的差不多，但是支持单独使用如： 12-&gt;搜索包含tom的文档+tom - 排除该操作符后跟着的搜索条件，效果类似NOT，如： 12=&gt;搜索不包含tom的文档-tom 分组（Grouping） 支持使用小括号对每个子句进行分组，形成更为复杂的查询逻辑。 12-&gt;要搜索包含hello的文档中，也包含tom或者john的hello AND (tom OR john) 转义字符（Escaping Special Character） 由于Lucene中支持很多的符号，如： 1+ - &amp;&amp; || ! ( ) &#123; &#125; [ ] ^ &quot; ~ * ? : \\ 因此如果需要搜索(1+1):2需要对改串进行转换，使用字符\\ 1\\(1\\+1\\)\\:2","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"}]},{"title":"Solr使用教程","slug":"Solr使用教程","date":"2019-07-17T09:55:32.000Z","updated":"2019-07-25T09:14:17.130Z","comments":true,"path":"2019/07/17/Solr使用教程/","link":"","permalink":"http://yoursite.com/2019/07/17/Solr使用教程/","excerpt":"","text":"能够检索相关信息是许多应用的基本要求，一些开发者和架构师通过 复杂的SQL请求来检索数据 在非关系型数据中进行检索 Solr可以作为一个搜索引擎使用 Apache Solr是一个流行的，快速的，开源的企业级搜索平台，应用在很多大型网站 中。 Apache Solr安装 有Java依赖，安装前先配置好java环境变量。 从Solr官网下载二进制安装包 解压包，进步bin目录 执行sudo bash ./install_solr_service.sh solr.zip -i /opt -d /var/solr -u solr -s solr -p 8983 安装脚本只认tgz和zip文件，若已经安装过solr，需加-f参数表示更新solr。","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"}]},{"title":"Python读取导出json格式数据文件","slug":"Python读取导出json格式数据文件","date":"2019-05-15T06:38:23.000Z","updated":"2019-07-25T09:14:05.790Z","comments":true,"path":"2019/05/15/Python读取导出json格式数据文件/","link":"","permalink":"http://yoursite.com/2019/05/15/Python读取导出json格式数据文件/","excerpt":"JSON(JavaScript Object Notation)是一种轻量级的数据交换格式。它基于ECMAScript的一个子集。 JSON采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯(包括C、C++、Java、 JavaScript、Perl、Python 等)。这些特性使JSON成为理想的数据交换语言。易于人阅读和编写，同时也 易于机器解析和生成(一般用于提升网络传输速率)。","text":"JSON(JavaScript Object Notation)是一种轻量级的数据交换格式。它基于ECMAScript的一个子集。 JSON采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯(包括C、C++、Java、 JavaScript、Perl、Python 等)。这些特性使JSON成为理想的数据交换语言。易于人阅读和编写，同时也 易于机器解析和生成(一般用于提升网络传输速率)。 在Python中有默认的模块实现json数据的导入导出。 导入json模块 1import json 将Python字典转换为字符串 1234# 自定义一个python字典数据test_dict = &#123;'bigberg': [7600, &#123;1: [['iPhone', 6300], ['Bike', 800], ['shirt', 300]]&#125;]&#125;# 将字典转换为json字符串json_str = json.dumps(test_dict) 将字符串转换为Python字典 12# 将json字符串还原为字典new_dict = json.loads(json_str) 将Python字典写入json格式文件中 123# 将字典new_dict中的数据写入json格式文件中with open(\"record.json\",\"w\") as f: json.dump(new_dict,f) 从json格式文件中导入数据到Python字典 123with open(\"record.json\",'r') as load_f: load_dict = json.load(load_f) print(load_dict)","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"线性模型","slug":"线性模型","date":"2018-08-23T04:38:12.000Z","updated":"2019-07-25T09:13:54.132Z","comments":true,"path":"2018/08/23/线性模型/","link":"","permalink":"http://yoursite.com/2018/08/23/线性模型/","excerpt":"一. 基本形式 由$d$个属性描述的实例$x=(x_1; x_2; ...; x_d)$，线性回归试图学得一个通过属性的线性组合来预测的函数： $$ f(x)=w_1 x_1 + w_2 x_2 + ... + w_d x_d + b $$","text":"一. 基本形式 由$d$个属性描述的实例$x=(x_1; x_2; ...; x_d)$，线性回归试图学得一个通过属性的线性组合来预测的函数： $$ f(x)=w_1 x_1 + w_2 x_2 + ... + w_d x_d + b $$ 向量形式写成： $ f(x)=w^T x+b$ ，其中 $w = (w_1; w_2; ...; w_d)$ 二. 线性回归 设样本集的数目为$m$，$x$代表输入变量（特征），$y$代表输出变量（目标），$(x,y)$代表样本集中的实例，$(x^i,y^i)$代表样本集中第$i$个实例，函数$f(x)$是模型的假设。 1. 损失函数 模型的参数$w, b$决定模型预测能力的好坏，模型在训练集中的预测值与实际值的差距是我们的建模误差，代价函数即定义为建模误差的平方和： $ J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m}(f(x^i)-y^i)^2 $ 我们的目标是选择使代价函数最小的模型参数。 2. 梯度下降法 $w = w - \\alpha \\frac{\\partial}{\\partial w}J(w,b)$ $b = b - \\alpha \\frac{\\partial}{\\partial b}J(w,b)$ $\\alpha$ 是学习率 $\\frac{\\partial}{\\partial b}J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}(f(x^i)-y^i)$ $\\frac{\\partial}{\\partial w}J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}((f(x^i)-y^i) x^i)$ 3. 最小二乘法 三. 逻辑回归 逻辑回归用来处理$y$值是离散情况的分类问题。 1. 分类问题 2. 假说表示 3. 判定边界 4. 代价函数与梯度下降 四. 正则化","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"MNIST数据集","slug":"MNIST数据集","date":"2018-08-23T02:21:00.000Z","updated":"2019-07-25T09:13:15.952Z","comments":true,"path":"2018/08/23/MNIST数据集/","link":"","permalink":"http://yoursite.com/2018/08/23/MNIST数据集/","excerpt":"MNIST 数据集可在 http://yann.lecun.com/exdb/mnist/ 获取，它包含了四个部分：","text":"MNIST 数据集可在 http://yann.lecun.com/exdb/mnist/ 获取，它包含了四个部分： Training set images： train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本) Training set labels： train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签) Test set images： t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本) Test set labels： t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签) MNIST 数据集来自美国国家标准与技术研究所，National Institute of Standards and Technology (NIST)，训练集 (training set) 由来自 250 个不同人手写的数字构成，其中 50% 是高中学生，50% 来自人口普查局 (the Census Bureau) 的工作人员。测试集(test set) 也是同样比例的手写数字数据。 图片是以字节的形式进行存储, 我们需要把它们读取到 NumPy array 中，以便训练和测试算法。 12345678910111213141516171819202122232425import osimport structimport numpy as npdef load_mnist(path, kind='train'): \"\"\"Load MNIST data from `path`\"\"\" labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind) images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind) with open(labels_path, 'rb') as lbpath: magic, n = struct.unpack('&gt;II', lbpath.read(8)) labels = np.fromfile(lbpath, dtype=np.uint8) with open(images_path, 'rb') as imgpath: magic, num, rows, cols = struct.unpack('&gt;IIII', imgpath.read(16)) images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784) return images, labels load_mnist 函数返回两个数组，第一个是一个 n x m 维的 NumPy array(images)，这里的 n 是样本数(行数)，m 是特征数(列数)。训练数据集包含 60000 个样本，测试数据集包含 10000 样本。在 MNIST 数据集中的每张图片由 28 x 28 个像素点构成，每个像素点用一个灰度值表示。在这里, 我们将 28 x 28 的像素展开为一个一维的行向量，这些行向量就是图片数组里的行(每行 784 个值，或者说每行就是代表了一张图片)。 load_mnist 函数返回的第二个数组(labels) 包含了相应的目标变量，也就是手写数字的类标签(整数 0-9)。 通过执行上面的代码，我们将会从刚刚解压 MNIST 数据集后的 mnist 目录下加载 60000 个训练样本和 10000 个测试样本。 为了了解 MNIST 中的图片看起来到底是个啥，让我们来对它们进行可视化处理。从 feature matrix 中将 784-像素值的向量 reshape 为之前的 28*28 的形状，然后通过 matplotlib 的 imshow 函数进行绘制： 1234567891011121314151617import matplotlib.pyplot as pltfig, ax = plt.subplots( nrows=2, ncols=5, sharex=True, sharey=True, )ax = ax.flatten()for i in range(10): img = X_train[y_train == i][0].reshape(28, 28) ax[i].imshow(img, cmap='Greys', interpolation='nearest')ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show() 此外，我们还可以绘制某一数字的多个样本图片，来看一下这些手写样本到底有多不同： 123456789101112131415fig, ax = plt.subplots( nrows=5, ncols=5, sharex=True, sharey=True, )ax = ax.flatten()for i in range(25): img = X_train[y_train == 7][i].reshape(28, 28) ax[i].imshow(img, cmap='Greys', interpolation='nearest')ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show()","categories":[],"tags":[{"name":"数据集","slug":"数据集","permalink":"http://yoursite.com/tags/数据集/"}]},{"title":"使用Pandas模块读取csv格式文件","slug":"使用Pandas模块读取csv格式文件","date":"2018-08-22T08:34:09.000Z","updated":"2019-07-25T09:13:03.195Z","comments":true,"path":"2018/08/22/使用Pandas模块读取csv格式文件/","link":"","permalink":"http://yoursite.com/2018/08/22/使用Pandas模块读取csv格式文件/","excerpt":"在数据处理中，常遇到csv格式的文件，下面简要介绍如何使用Python中的Pandas模块来 读取csv文件中的数据。","text":"在数据处理中，常遇到csv格式的文件，下面简要介绍如何使用Python中的Pandas模块来 读取csv文件中的数据。 一. CSV文件 CSV(Comma-Separated Values)文件以纯文本形式存储表格数据，文件由任意数目的记录 组成，记录间以换行符分隔，每条记录由字段组成，字段间的分隔符可自定义，通常是 逗号。下面的数据取自Kaggle中Titanic的乘客信息数据。 1234567891011PassengerId,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked892,3,&quot;Kelly, Mr. James&quot;,male,34.5,0,0,330911,7.8292,,Q893,3,&quot;Wilkes, Mrs. James (Ellen Needs)&quot;,female,47,1,0,363272,7,,S894,2,&quot;Myles, Mr. Thomas Francis&quot;,male,62,0,0,240276,9.6875,,Q895,3,&quot;Wirz, Mr. Albert&quot;,male,27,0,0,315154,8.6625,,S896,3,&quot;Hirvonen, Mrs. Alexander (Helga E Lindqvist)&quot;,female,22,1,1,3101298,12.2875,,S897,3,&quot;Svensson, Mr. Johan Cervin&quot;,male,14,0,0,7538,9.225,,S898,3,&quot;Connolly, Miss. Kate&quot;,female,30,0,0,330972,7.6292,,Q899,2,&quot;Caldwell, Mr. Albert Francis&quot;,male,26,1,1,248738,29,,S900,3,&quot;Abrahim, Mrs. Joseph (Sophie Halaut Easu)&quot;,female,18,0,0,2657,7.2292,,C901,3,&quot;Davies, Mr. John Samuel&quot;,male,21,2,0,A/4 48871,24.15,,S 二. Pandas模块 导入pandas模块 1import pandas as pd 读取csv文件中的数据 1data = pd.read_csv(\"train.csv\",sep=',') 参数sep设定csv文件中分隔符，默认为,。 数据集对应的参数名 1data.columns 第 $i$ 条记录 1data.iloc[i-1] 数据集的大小 1data.shape 数据集的描述 1data.describe 数据集参数数组里的不同值 1titanic['Sex'].unique() 数据集字符串到数字的映射 12titanic.loc[titanic['Sex']=='male','Sex']=0titanic.loc[titanic['Sex']=='female','Sex']=1 数据集中缺失数据的补填 1titanic['Age']=titanic['Age'].fillna(titanic['Age'].median())","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]}]}